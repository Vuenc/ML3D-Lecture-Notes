<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>ML3D summary</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      word-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <style>
  /*! normalize.css v3.0.1 | MIT License | git.io/normalize */

  /**
  * 1. Set default font family to sans-serif.
  * 2. Prevent iOS text size adjust after orientation change, without disabling
  *    user zoom.
  */

  html {
    font-family: sans-serif; /* 1 */
    -ms-text-size-adjust: 100%; /* 2 */
    -webkit-text-size-adjust: 100%; /* 2 */
  }

  /**
  * Remove default margin.
  */

  body {
    margin: 0;
  }

  /* HTML5 display definitions
  ========================================================================== */

  /**
  * Correct `block` display not defined for any HTML5 element in IE 8/9.
  * Correct `block` display not defined for `details` or `summary` in IE 10/11 and Firefox.
  * Correct `block` display not defined for `main` in IE 11.
  */

  article,
  aside,
  details,
  figcaption,
  figure,
  footer,
  header,
  hgroup,
  main,
  nav,
  section,
  summary {
    display: block;
  }

  /**
  * 1. Correct `inline-block` display not defined in IE 8/9.
  * 2. Normalize vertical alignment of `progress` in Chrome, Firefox, and Opera.
  */

  audio,
  canvas,
  progress,
  video {
    display: inline-block; /* 1 */
    vertical-align: baseline; /* 2 */
  }

  /**
  * Prevent modern browsers from displaying `audio` without controls.
  * Remove excess height in iOS 5 devices.
  */

  audio:not([controls]) {
    display: none;
    height: 0;
  }

  /**
  * Address `[hidden]` styling not present in IE 8/9/10.
  * Hide the `template` element in IE 8/9/11, Safari, and Firefox < 22.
  */

  [hidden],
  template {
    display: none;
  }

  /* Links
  ========================================================================== */

  /**
  * Remove the gray background color from active links in IE 10.
  */

  a {
    background: transparent;
  }

  /**
  * Improve readability when focused and also mouse hovered in all browsers.
  */

  a:active,
  a:hover {
    outline: 0;
  }

  /* Text-level semantics
  ========================================================================== */

  /**
  * Address styling not present in IE 8/9/10/11, Safari, and Chrome.
  */

  abbr[title] {
    border-bottom: 1px dotted;
  }

  /**
  * Address style set to `bolder` in Firefox 4+, Safari, and Chrome.
  */

  b,
  strong {
    font-weight: bold;
  }

  /**
  * Address styling not present in Safari and Chrome.
  */

  dfn {
    font-style: italic;
  }

  /**
  * Address variable `h1` font-size and margin within `section` and `article`
  * contexts in Firefox 4+, Safari, and Chrome.
  */

  h1 {
    font-size: 2em;
    margin: 0.67em 0;
  }

  /**
  * Address styling not present in IE 8/9.
  */

  mark {
    background: #ff0;
    color: #000;
  }

  /**
  * Address inconsistent and variable font size in all browsers.
  */

  small {
    font-size: 80%;
  }

  /**
  * Prevent `sub` and `sup` affecting `line-height` in all browsers.
  */

  sub,
  sup {
    font-size: 75%;
    line-height: 0;
    position: relative;
    vertical-align: baseline;
  }

  sup {
    top: -0.5em;
  }

  sub {
    bottom: -0.25em;
  }

  /* Embedded content
  ========================================================================== */

  /**
  * Remove border when inside `a` element in IE 8/9/10.
  */

  img {
    border: 0;
  }

  /**
  * Correct overflow not hidden in IE 9/10/11.
  */

  svg:not(:root) {
    overflow: hidden;
  }

  /* Grouping content
  ========================================================================== */

  /**
  * Address margin not present in IE 8/9 and Safari.
  */

  figure {
    margin: 1em 40px;
  }

  /**
  * Address differences between Firefox and other browsers.
  */

  hr {
    -moz-box-sizing: content-box;
    box-sizing: content-box;
    height: 0;
  }

  /**
  * Contain overflow in all browsers.
  */

  pre {
    overflow: auto;
  }

  /**
  * Address odd `em`-unit font size rendering in all browsers.
  */

  code,
  kbd,
  pre,
  samp {
    font-family: monospace, monospace;
    font-size: 1em;
  }

  /* Forms
  ========================================================================== */

  /**
  * Known limitation: by default, Chrome and Safari on OS X allow very limited
  * styling of `select`, unless a `border` property is set.
  */

  /**
  * 1. Correct color not being inherited.
  *    Known issue: affects color of disabled elements.
  * 2. Correct font properties not being inherited.
  * 3. Address margins set differently in Firefox 4+, Safari, and Chrome.
  */

  button,
  input,
  optgroup,
  select,
  textarea {
    color: inherit; /* 1 */
    font: inherit; /* 2 */
    margin: 0; /* 3 */
  }

  /**
  * Address `overflow` set to `hidden` in IE 8/9/10/11.
  */

  button {
    overflow: visible;
  }

  /**
  * Address inconsistent `text-transform` inheritance for `button` and `select`.
  * All other form control elements do not inherit `text-transform` values.
  * Correct `button` style inheritance in Firefox, IE 8/9/10/11, and Opera.
  * Correct `select` style inheritance in Firefox.
  */

  button,
  select {
    text-transform: none;
  }

  /**
  * 1. Avoid the WebKit bug in Android 4.0.* where (2) destroys native `audio`
  *    and `video` controls.
  * 2. Correct inability to style clickable `input` types in iOS.
  * 3. Improve usability and consistency of cursor style between image-type
  *    `input` and others.
  */

  button,
  html input[type="button"], /* 1 */
  input[type="reset"],
  input[type="submit"] {
    -webkit-appearance: button; /* 2 */
    cursor: pointer; /* 3 */
  }

  /**
  * Re-set default cursor for disabled elements.
  */

  button[disabled],
  html input[disabled] {
    cursor: default;
  }

  /**
  * Remove inner padding and border in Firefox 4+.
  */

  button::-moz-focus-inner,
  input::-moz-focus-inner {
    border: 0;
    padding: 0;
  }

  /**
  * Address Firefox 4+ setting `line-height` on `input` using `!important` in
  * the UA stylesheet.
  */

  input {
    line-height: normal;
  }

  /**
  * It's recommended that you don't attempt to style these elements.
  * Firefox's implementation doesn't respect box-sizing, padding, or width.
  *
  * 1. Address box sizing set to `content-box` in IE 8/9/10.
  * 2. Remove excess padding in IE 8/9/10.
  */

  input[type="checkbox"],
  input[type="radio"] {
    box-sizing: border-box; /* 1 */
    padding: 0; /* 2 */
  }

  /**
  * Fix the cursor style for Chrome's increment/decrement buttons. For certain
  * `font-size` values of the `input`, it causes the cursor style of the
  * decrement button to change from `default` to `text`.
  */

  input[type="number"]::-webkit-inner-spin-button,
  input[type="number"]::-webkit-outer-spin-button {
    height: auto;
  }

  /**
  * 1. Address `appearance` set to `searchfield` in Safari and Chrome.
  * 2. Address `box-sizing` set to `border-box` in Safari and Chrome
  *    (include `-moz` to future-proof).
  */

  input[type="search"] {
    -webkit-appearance: textfield; /* 1 */
    -moz-box-sizing: content-box;
    -webkit-box-sizing: content-box; /* 2 */
    box-sizing: content-box;
  }

  /**
  * Remove inner padding and search cancel button in Safari and Chrome on OS X.
  * Safari (but not Chrome) clips the cancel button when the search input has
  * padding (and `textfield` appearance).
  */

  input[type="search"]::-webkit-search-cancel-button,
  input[type="search"]::-webkit-search-decoration {
    -webkit-appearance: none;
  }

  /**
  * Define consistent border, margin, and padding.
  */

  fieldset {
    border: 1px solid #c0c0c0;
    margin: 0 2px;
    padding: 0.35em 0.625em 0.75em;
  }

  /**
  * 1. Correct `color` not being inherited in IE 8/9/10/11.
  * 2. Remove padding so people aren't caught out if they zero out fieldsets.
  */

  legend {
    border: 0; /* 1 */
    padding: 0; /* 2 */
  }

  /**
  * Remove default vertical scrollbar in IE 8/9/10/11.
  */

  textarea {
    overflow: auto;
  }

  /**
  * Don't inherit the `font-weight` (applied by a rule above).
  * NOTE: the default cannot safely be changed in Chrome and Safari on OS X.
  */

  optgroup {
    font-weight: bold;
  }

  /* Tables
  ========================================================================== */

  /**
  * Remove most spacing between table cells.
  */

  table {
    border-collapse: collapse;
    border-spacing: 0;
  }

  td,
  th {
    padding: 0;
  }

  /* Figure captions */

  .caption {
    margin-bottom: 5em;
  }

  /*! End of normalize.css
  ========================================================================== */

  /* Github styles
  ========================================================================== */

  * {
    box-sizing: border-box;
  }
  body {
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
    font-size: 12pt;
    line-height: 1.6;
    margin: auto;
    max-width: 920px;
    min-width: 360px;
    padding: 2rem;
    word-wrap: break-word;
  }

  /* Headers
  ========================================================================== */

  h1, h2, h3, h4, h5, h6 {
    font-weight: 600;
    line-height: 1.25;
    margin-bottom: 16px;
    margin-top: 24px;
  }
  h1 {
    padding-bottom: 0.3em;
    font-size: 2em;
    border-bottom: 1px solid #eee;
  }
  h2 {
    padding-bottom: 0.3em;
    font-size: 1.5em;
    border-bottom: 1px solid #eee;
  }
  h3 {
    font-size: 1.25em;;
  }
  h4 {
    font-size: 1em;
  }
  h5 {
    font-size: 0.875em;
  }
  h6 {
    font-size: 0.85em;
    color: #777;
  }
  h1 tt, h1 code,
  h2 tt, h2 code,
  h3 tt, h3 code,
  h4 tt, h4 code,
  h5 tt, h5 code,
  h6 tt, h6 code {
    font-size: inherit;
  }
  body > h2:first-child {
    margin-top: 0;
    padding-top: 0;
  }
  body > h1:first-child {
    margin-top: 0;
    padding-top: 0;
  }
  body > h1:first-child+h2 {
    margin-top: 0;
    padding-top: 0;
  }
  body > h3:first-child,
  body > h4:first-child,
  body > h5:first-child,
  body > h6:first-child {
    margin-top: 0;
    padding-top: 0;
  }
  a:first-child h1,
  a:first-child h2,
  a:first-child h3,
  a:first-child h4,
  a:first-child h5,
  a:first-child h6 {
    margin-top: 0;
    padding-top: 0;
  }

  /* Flow Content
  ========================================================================== */

  a {
    color: #4078c0;
    text-decoration: none;
  }
  a:active, a:hover {
    outline: 0;
    text-decoration: underline;
  }
  sup, sub, a.footnote {
    font-size: 75%;
    line-height: 0;
    position: relative;
    vertical-align: baseline;
  }
  sub {
    bottom: -0.25em;
  }
  sup {
    top: -0.5em;
  }

  /* Block Content
  ========================================================================== */

  ol, ul {
    margin-bottom: 16px;
    margin-top: 0;
    padding-left: 2em;
  }
  p, blockquote, table, pre {
    margin: 15px 0;
  }
  ul, ol {
    padding-left: 2em;
  }
  ul.no-list, ol.no-list {
    padding: 0;
    list-style-type: none;
  }
  ul ul, ul ol, ol ol, ol ul {
    margin-top: 0;
    margin-bottom: 0;
  }
  li > p {
    margin-top: 16px;
  }
  li + li {
    margin-top: 0.25em;;
  }
  ol li ul:first-of-type {
    margin-top: 0;
  }
  hr {
    background: transparent url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAYAAAAECAYAAACtBE5DAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAyJpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNSBNYWNpbnRvc2giIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OENDRjNBN0E2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OENDRjNBN0I2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo4Q0NGM0E3ODY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo4Q0NGM0E3OTY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PqqezsUAAAAfSURBVHjaYmRABcYwBiM2QSA4y4hNEKYDQxAEAAIMAHNGAzhkPOlYAAAAAElFTkSuQmCC) repeat-x 0 0;
    border: 0 none;
    color: #ccc;
    height: 4px;
    margin: 16px 0;
    padding: 0;
  }
  h1 + p,
  h2 + p,
  h3 + p,
  h4 + p,
  h5 + p,
  h6 + p,
  ul li > :first-child,
  ol li > :first-child {
    margin-top: 0;
  }
  dl {
    padding: 0;
  }
  dl dt {
    font-size: 1em;
    font-weight: bold;
    font-style: italic;
    padding: 0;
    margin: 15px 0 5px;
  }
  dl dt:first-child {
    padding: 0;
  }
  dl dt > :first-child {
    margin-top: 0;
  }
  dl dt > :last-child {
    margin-bottom: 0;
  }
  dl dd {
    margin: 0 0 15px;
    padding: 0 15px;
  }
  dl dd > :first-child {
    margin-top: 0;
  }
  dl dd > :last-child {
    margin-bottom: 0;
  }
  blockquote {
    border-left: 4px solid #DDD;
    padding: 0 15px;
    color: #777;
  }
  blockquote > :first-child {
    margin-top: 0;
  }
  blockquote > :last-child {
    margin-bottom: 0;
  }
  table {
    border-collapse: collapse;
    border-spacing: 0;
    font-size: 100%;
    font: inherit;
  }
  table th {
    font-weight: bold;
    border: 1px solid #ccc;
    padding: 6px 13px;
  }
  table td {
    border: 1px solid #ccc;
    padding: 6px 13px;
  }
  table td > p:first-child {
    margin-top: 0;
  }
  table td > p:last-child {
    margin-bottom: 0;
  }
  table tr {
    border-top: 1px solid #ccc;
    background-color: #fff;
  }
  table tr:nth-child(2n) {
    background-color: #f8f8f8;
  }
  img {
    max-width: 100%;
  }

  /* Code
  ========================================================================== */

  code, tt {
    padding: 0;
    padding-top: 0.2em;
    padding-bottom: 0.2em;
    margin: 0;
    font-family: Consolas, 'Liberation Mono', Menlo, Courier, monospace;
    font-size: 10pt;
    background-color: rgba(0, 0, 0, 0.04);
    border-radius: 3px;
  }
  pre > code {
    margin: 0;
    padding: 0;
    white-space: pre;
    border: 0;
    background: transparent;
  }
  pre {
    padding: 16px;
    margin-top: 0;
    margin-bottom: 0;
    background-color: #f8f8f8;
    font-size: 10pt;
    line-height: 19px;
    overflow: auto;
    border-radius: 3px;
  }
  pre code, pre tt {
    background-color: transparent;
    border: 0;
  }
  code::before, code::after,
  tt::before, tt::after {
    letter-spacing: -0.2em;
  	content: "\00a0";
  }
  pre code::before, pre code::after, pre tt::before, pre tt::after {
  	content: normal;
  }
  code > span.kw { color: #a71d5d; font-weight: normal; } /* Keyword */
  code > span.dt { color: inherit; } /* DataType */
  code > span.dv { color: #0086b3; } /* DecVal */
  code > span.bn { color: #0086b3; } /* BaseN */
  code > span.fl { color: #0086b3; } /* Float */
  code > span.ch { color: #183691; } /* Char */
  code > span.st { color: #183691; } /* String */
  code > span.co { color: #969896; font-style: normal; } /* Comment */
  code > span.ot { color: #a71d5d; } /* Other */
  code > span.al { color: #ff0000; } /* Alert */
  code > span.fu { color: #795da3; } /* Function */
  code > span.er { color: #ff0000; font-weight: bold; } /* Error */
  code > span.wa { color: #969896; font-weight: bold; font-style: italic; } /* Warning */
  code > span.cn { color: #880000; } /* Constant */
  code > span.sc { color: #183691; } /* SpecialChar */
  code > span.vs { color: #183691; } /* VerbatimString */
  code > span.ss { color: #bb6688; } /* SpecialString */
  code > span.im { } /* Import */
  code > span.va { color: #19177c; } /* Variable */
  code > span.cf { color: #a71d5d; font-weight: normal; } /* ControlFlow */
  code > span.op { color: #666666; } /* Operator */
  code > span.bu { } /* BuiltIn */
  code > span.ex { } /* Extension */
  code > span.pp { color: #bc7a00; } /* Preprocessor */
  code > span.at { color: #0086b3; } /* Attribute */
  code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
  code > span.an { color: #969896; font-weight: bold; font-style: italic; } /* Annotation */
  code > span.cv { color: #969896; font-weight: bold; font-style: italic; } /* CommentVar */
  code > span.in { color: #969896; font-weight: bold; font-style: italic; } /* Information */

  /* Print
  ========================================================================== */

  @media print {
    body {
      background: #fff;
    }
    img, pre, blockquote, table, figure {
      page-break-inside: avoid;
    }
    body {
      background: #fff;
      border: 0;
    }
    code {
      background-color: #fff;
      color: #333!important;
      padding: 0 .2em;
      border: 1px solid #dedede;
    }
    pre {
      background: #fff;
    }
    pre code {
      background-color: white!important;
      overflow: visible;
    }
  }

  /* Selection
  ========================================================================== */

  @media screen {
    ::selection {
      background: rgba(157, 193, 200, 0.5);
    }
    h1::selection {
      background-color: rgba(45, 156, 208, 0.3);
    }
    h2::selection {
      background-color: rgba(90, 182, 224, 0.3);
    }
    h3::selection, h4::selection, h5::selection, h6::selection, li::selection, ol::selection {
      background-color: rgba(133, 201, 232, 0.3);
    }
    code::selection {
      background-color: rgba(0, 0, 0, 0.7);
      color: #eee;
    }
    code span::selection {
      background-color: rgba(0, 0, 0, 0.7)!important;
      color: #eee!important;
    }
    a::selection {
      background-color: rgba(255, 230, 102, 0.2);
    }
    .inverted a::selection {
      background-color: rgba(255, 230, 102, 0.6);
    }
    td::selection, th::selection, caption::selection {
      background-color: rgba(180, 237, 95, 0.5);
    }
  }
  </style>
</head>
<body>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#geometric-foundations-surface-representations">01 - Geometric Foundations: Surface Representations</a>
<ul>
<li><a href="#voxel-grids">Voxel grids</a></li>
<li><a href="#point-clouds">Point clouds</a></li>
<li><a href="#polygontriangle-meshes">Polygon/Triangle meshes</a></li>
<li><a href="#parametric-surfaces-and-curves">Parametric surfaces and curves</a></li>
<li><a href="#implicit-surfaces">Implicit surfaces</a></li>
<li><a href="#conversion-between-representations">Conversion between representations</a></li>
<li><a href="#geometric-operators">Geometric operators</a></li>
<li><a href="#useful-software">Useful Software</a></li>
</ul></li>
<li><a href="#shapes-alignment-descriptors-similarity">02 - Shapes: Alignment, Descriptors, Similarity</a>
<ul>
<li><a href="#d-shape-alignment-registration">3D Shape Alignment (<em>Registration</em>)</a></li>
<li><a href="#shape-descriptors">Shape Descriptors</a></li>
<li><a href="#non-rigid-shape-matching">Non-Rigid shape matching</a></li>
<li><a href="#shape-search">Shape Search</a></li>
</ul></li>
<li><a href="#machine-learning-foundations">03 - Machine Learning Foundations</a></li>
<li><a href="#shape-segmentation-and-labeling">04 - Shape Segmentation and Labeling</a>
<ul>
<li><a href="#d-classification-tasks">3D Classification tasks</a></li>
<li><a href="#d-shape-segmentation-tasks">3D Shape segmentation tasks</a></li>
<li><a href="#unsupervised-co-segmentation">Unsupervised Co-Segmentation</a></li>
<li><a href="#active-learning-human-in-the-loop">Active Learning: Human-in-the-loop</a></li>
<li><a href="#datasets-for-shape-segmentation">Datasets for Shape Segmentation</a></li>
</ul></li>
<li><a href="#shape-generation">05 - Shape Generation</a>
<ul>
<li><a href="#example-usecases">Example usecases</a></li>
<li><a href="#shape-reconstruction">Shape Reconstruction</a></li>
<li><a href="#point-cloud-generation">Point Cloud Generation</a></li>
<li><a href="#parametric-3d-model-generation-smirnov-et-al.-21">Parametric 3D model generation [Smirnov et al. ’21]</a></li>
<li><a href="#reconstructing-explicit-3d-meshes">Reconstructing Explicit 3D Meshes</a></li>
<li><a href="#joint-embedding-for-retrieval">Joint Embedding for Retrieval</a></li>
</ul></li>
<li><a href="#learning-on-different-3d-representations">06 - Learning on Different 3D Representations</a>
<ul>
<li><a href="#types-of-3d-represenations">Types of 3D Represenations</a></li>
<li><a href="#volumetric-grids">Volumetric Grids</a></li>
<li><a href="#multi-view-representations">Multi-View Representations</a></li>
<li><a href="#handling-irregular-structures">Handling Irregular Structures</a></li>
<li><a href="#meshes">Meshes</a></li>
<li><a href="#combining-representations">Combining Representations</a></li>
</ul></li>
<li><a href="#semantic-scene-segmentation">07 - Semantic Scene Segmentation</a>
<ul>
<li><a href="#popular-benchmarks">Popular Benchmarks</a></li>
<li><a href="#scene-segmentation-vs.-part-segmentation">Scene segmentation vs. Part segmentation</a></li>
<li><a href="#d-inputs-vs-2d-inputs">3D inputs vs 2D inputs</a></li>
<li><a href="#solving-size-problem">Solving size problem</a></li>
<li><a href="#convolutions-to-allow-varying-input-sizes">Convolutions to allow varying input sizes</a></li>
<li><a href="#allowing-higher-resolution">Allowing higher resolution</a></li>
<li><a href="#non-regular-grid-geometry">Non-Regular Grid Geometry</a></li>
<li><a href="#online-semantic-segmentation">Online Semantic Segmentation</a></li>
<li><a href="#multi-view3d-geometry-fusion-revisited">Multi-View/3D Geometry Fusion Revisited</a></li>
</ul></li>
<li><a href="#object-detection-instance-segmentation">08 - Object Detection + Instance Segmentation</a>
<ul>
<li><a href="#understanding-object-ness">Understanding Object-ness</a></li>
<li><a href="#d-object-detection-bounding-boxes">3D Object Detection (Bounding Boxes)</a></li>
<li><a href="#first-approaches-exploit-spatial-separation">First Approaches: exploit spatial separation</a></li>
<li><a href="#d-sis-semantic-instance-segmentation-hou-et-al.-19">3D-SIS (Semantic Instance Segmentation) [Hou et al. ’19]</a></li>
<li><a href="#revealnet-hou-et-al.-20">RevealNet [Hou et al. ’20]</a></li>
<li><a href="#summary-top-down-anchor-based-mask-r-cnn-style-approaches">Summary: Top-down, anchor-based (Mask-R-CNN-style) approaches</a></li>
<li><a href="#point-cloud-object-detection">Point Cloud Object detection</a></li>
<li><a href="#towards-part-based-scene-understanding">Towards Part-Based Scene Understanding</a></li>
</ul></li>
<li><a href="#reconstructing-and-generating-scenes">09 - Reconstructing and Generating Scenes</a>
<ul>
<li><a href="#generative-tasks">Generative Tasks</a></li>
<li><a href="#excursion-generative-tasks-in-2d">Excursion: Generative tasks in 2D</a></li>
<li><a href="#scan-completionsurface-reconstruction-tasks-both-appear-in-this-section">Scan completion/Surface reconstruction tasks (both appear in this section)</a></li>
<li><a href="#scene-synthesis-task">Scene synthesis task</a></li>
<li><a href="#textured-scene-generation">Textured Scene Generation</a></li>
</ul></li>
<li><a href="#functional-analysis-of-scenes">10 - Functional Analysis of Scenes</a>
<ul>
<li><a href="#subtask-predicting-interactions">Subtask: Predicting interactions</a></li>
<li><a href="#human-motion-extraction">Human motion extraction</a></li>
<li><a href="#generating-human-object-interaction-snapshots">Generating Human-Object interaction snapshots</a></li>
<li><a href="#predicting-human-motion">Predicting human motion</a></li>
<li><a href="#simulation-environments">Simulation Environments</a></li>
</ul></li>
<li><a href="#weak-supervision-n-shot-learning-data-efficiency">11 - Weak Supervision, n-shot Learning, Data Efficiency</a>
<ul>
<li><a href="#training-methods-and-required-amount-of-labeled-data">Training methods and required amount of labeled data</a></li>
<li><a href="#few-shot-learning">Few-shot learning</a></li>
<li><a href="#generalizing-accross-datasets">Generalizing accross datasets</a></li>
<li><a href="#approaches-with-lessno-supervision">Approaches with less/no supervision</a></li>
<li><a href="#domain-adaption">Domain Adaption</a></li>
</ul></li>
</ul>
</nav>
<h1 id="geometric-foundations-surface-representations">01 - Geometric Foundations: Surface Representations</h1>
<p>There are several ways to represent 3D shapes, each with their own advantages and disadvantages. Aspects to consider are the memory usage, how efficient operations are, and the constraints of the source data and target application.</p>
<h3 id="voxel-grids">Voxel grids</h3>
<p>Voxels = Pixels in 3D. Each Voxel stores an attribute like occupancy (boolean), distance from the object (SDF), colors, etc.</p>
<p>Advantages: arbitrary topologies, easy to query, easy to operate on neighbors.</p>
<p>Huge disadvantage: space requirement grows cubically. Sparse surfaces in occupancy grids lead to a lot of empty space (as the resolution grows, the ratio of occupied voxels goes to zero).</p>
<h3 id="point-clouds">Point clouds</h3>
<p>Set of <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20%28x%2C%20y%2C%20z%29" alt="(x, y, z)" title="(x, y, z)" class="math inline" /> locations of points (optionally, additional attributes). Unordered by definition. Can be the result of raw scanning data capture.</p>
<p>Advantage: More efficiently represents sparse surfaces.</p>
<p>Disadvantage: No spatial structure; less efficient neighbor queries.</p>
<h3 id="polygontriangle-meshes">Polygon/Triangle meshes</h3>
<p>Collection of <em>vertices</em>, (<em>edges</em>), and <em>faces</em>. Represent a <em>piecewise linear</em> approximation of a surface. Can approximate very closely if fine enough, though.</p>
<p>Advantages: arbitrary topologies, easy editing/manipulation, and easy rendering.</p>
<figure>
<img src="circle-approximations.png" alt="circle-approximations.png" /><figcaption aria-hidden="true">circle-approximations.png</figcaption>
</figure>
<p>Definition: a polygon mesh is a finite set of closed (i.e. end = start point) and simple (not self-intersecting) polygons. Each polygon defines a face of the polygonal mesh.</p>
<p><em>Boundary</em>: set of edges that belong to only one polygon. The boundary is either empty or consists of closed loops; if it is empty, the polygon mesh is closed.</p>
<p>Triangular meshes: polygons are triangulated. This simplies data structures/algorithms/rendering, since only triangles need to be considered.</p>
<p>Meshes can have additional attributes - for example, textured meshes.</p>
<h5 id="mesh-data-structures">Mesh data structures</h5>
<h6 id="stl-format-triangle-list-format">STL format (<em>Triangle List</em> format)</h6>
<p>Binary format, simply a list of triangles described by their corner coordinates: three bytes per coordinate, 9 coordinates per triangle =&gt; 36 bytes per triangle.</p>
<p>No connectivity information!</p>
<h6 id="obj-format-indexed-face-set-format">OBJ format (<em>Indexed Face Set</em> format)</h6>
<p>First, list of point coordinates (three numbers per line preceded by a <code>v</code>); then, list of faces (three or more indices per line, which refer to the points specified above, preceded by a <code>f</code>). More primitives, e.g. lines, also possible.</p>
<p>Other indexed face set formats: OFF, WRL</p>
<h3 id="parametric-surfaces-and-curves">Parametric surfaces and curves</h3>
<p>Functions <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20p%3A%20%5Cmathbb%7BR%7D%20%5Cto%20%5Cmathbb%7BR%7D%5E3" alt="p: \mathbb{R} \to \mathbb{R}^3" title="p: \mathbb{R} \to \mathbb{R}^3" class="math inline" /> (curves) or <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20p%3A%20%5Cmathbb%7BR%7D%5E2%20%5Cto%20%5Cmathbb%7BR%7D%5E3" alt="p: \mathbb{R}^2 \to \mathbb{R}^3" title="p: \mathbb{R}^2 \to \mathbb{R}^3" class="math inline" /> (surfaces). More advanced:</p>
<ul>
<li>Bezier curves</li>
<li>Splines</li>
<li>Bezier surfaces</li>
<li>Bicubic patches</li>
</ul>
<figure>
<img src="bezier-surface.png" alt="bezier-surface.png" /><figcaption aria-hidden="true">bezier-surface.png</figcaption>
</figure>
<p>Advantages of Bezier patch meshes:</p>
<ul>
<li>requires fewer points than triangle mesh</li>
<li>easy to manipulate: just transform the control points under a linear transformation to transform the whole mesh</li>
<li>easy to sample points</li>
<li>easy to ensure continuity</li>
</ul>
<p>Disadvantages:</p>
<ul>
<li>hard to determine if point is inside/outside/on surface</li>
<li>more complex rendering</li>
</ul>
<h3 id="implicit-surfaces">Implicit surfaces</h3>
<p>Implicit surfaces are represented by functions that assign values to points which relate to the surface.</p>
<h5 id="signed-distance-function">Signed distance function</h5>
<p><em>Signed distance function</em>: Function <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20f%3A%20%5Cmathbb%7BR%7D%5Em%20%5Cto%20%5Cmathbb%7BR%7D" alt="f: \mathbb{R}^m \to \mathbb{R}" title="f: \mathbb{R}^m \to \mathbb{R}" class="math inline" /> s.t. <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20f%28x%29%5C%3C%200" alt="f(x)\&lt; 0" title="f(x)\&lt; 0" class="math inline" /> on the inside, <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20f%28x%29%20%3E%200" alt="f(x) &gt; 0" title="f(x) &gt; 0" class="math inline" /> on the outside, and <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20f%28x%29%20%3D%200" alt="f(x) = 0" title="f(x) = 0" class="math inline" /> on the surface. Instead of using a function, one can also use a voxel grid with the SDF values filled in.</p>
<p>If we mostly care about values close to the surface: use a <em>truncated signed distance field</em> with N/A values far from the surface.</p>
<h5 id="operations-on-sdfs">Operations on SDFs</h5>
<p>Very efficient:</p>
<ul>
<li>union = <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20%5Cmin" alt="\min" title="\min" class="math inline" /> operation</li>
<li>intersection = <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20%5Cmax" alt="\max" title="\max" class="math inline" /> operation</li>
<li>subtraction = <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20%5Cmax%28f%2C%20-g%29" alt="\max(f, -g)" title="\max(f, -g)" class="math inline" /> operation</li>
</ul>
<figure>
<img src="sdf-operations.png" alt="sdf-operations.png" /><figcaption aria-hidden="true">sdf-operations.png</figcaption>
</figure>
<p>Advantages:</p>
<ul>
<li>easy operations</li>
<li>easy to determine if a point is inside/outside/on the surface Disadvantages:</li>
<li>hard to sample points on the surface!</li>
</ul>
<h3 id="conversion-between-representations">Conversion between representations</h3>
<h5 id="point-cloud---implicit-poisson-surface-reconstruction">Point cloud -&gt; Implicit: Poisson Surface Reconstruction</h5>
<p>Fit a function <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20f" alt="f" title="f" class="math inline" /> s.t. <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20f%20%5C%3C%200" alt="f \&lt; 0" title="f \&lt; 0" class="math inline" /> on the inside, <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20f%20%3E%200" alt="f &gt; 0" title="f &gt; 0" class="math inline" /> on the outside.</p>
<figure>
<img src="points-to-implicit.png" alt="300" /><figcaption aria-hidden="true">300</figcaption>
</figure>
<p>Poisson surface reconstruction [Kazhdan et al. ’06]: uses <em>oriented points</em> (i.e. points + surface normals) as inputs, computes an indicator function <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20%5Cchi_M" alt="\chi_M" title="\chi_M" class="math inline" /> (0 or 1). From the point normals, the <em>gradient</em> of <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20%5Cchi_M" alt="\chi_M" title="\chi_M" class="math inline" />, a vector field <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20V" alt="V" title="V" class="math inline" />, is computed. Then find a function <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20f" alt="f" title="f" class="math inline" /> whose gradient approximates <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20V" alt="V" title="V" class="math inline" />: Solve</p>
<p><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdisplaystyle%20%5Cmin_f%20%7C%7C%5Cnabla%20f%20-%20V%7C%7C" alt="\min_f ||\nabla f - V||" title="\min_f ||\nabla f - V||" class="math display" /></p>
<p>This can be transformed to a <em>Poisson problem</em> with the solutoin <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20%5CDelta%20f%20%3D%20%5Cnabla%20%2A%20V" alt="\Delta f = \nabla * V" title="\Delta f = \nabla * V" class="math inline" />, then solved as least-squares fitting problem. (not more details given)</p>
<h5 id="implicit---mesh-marching-cubes">Implicit -&gt; Mesh: Marching Cubes</h5>
<p>Extract the surface belonging to <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20f%20%3D%200" alt="f = 0" title="f = 0" class="math inline" /> in form of a mesh.</p>
<p>Marching cubes algorithm [Lorensen and Cline 1987]: Discretize space into voxel grid. For each cube, compute the implicit function at the 8 corners. This allows to approximate the <em>zero crossings</em> (i.e. where the surface crosses the cube surface).</p>
<p>Lookup configuration in lookup table (<img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%202%5E8" alt="2^8" title="2^8" class="math inline" /> possibilities depending on the 8 values at the edges).</p>
<figure>
<img src="marching-cubes-lookup.png" alt="marching-cubes-lookup.png" /><figcaption aria-hidden="true">marching-cubes-lookup.png</figcaption>
</figure>
<p>Improve by linearly interpolating the exact position on the cube edges (i.e. if the sdf is -1 on one corner and +10 on the other, the zero crossing should be closer to the first corner).</p>
<p>Advantages:</p>
<ul>
<li>widely applicable</li>
<li>easy to implement, trivial to parallelize</li>
</ul>
<p>Disadvantages:</p>
<ul>
<li>can create skinny triangles</li>
<li>lookup table with many special cases; some ambiguities are resolved arbitrarily</li>
<li>No sharp features</li>
</ul>
<h5 id="mesh---point-cloud">Mesh -&gt; Point cloud</h5>
<p>…to solve problems with bad triangles in meshes, etc.</p>
<p>Generate point cloud by sampling the mesh: Sample each triangle uniformly with barycentric coordinates, sample triangles with probability proportional to their area.</p>
<p>If <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20r_1%2C%20r_2%20%5Csim%20U%28%5C%5B0%2C%201%5C%5D%29" alt="r_1, r_2 \sim U(\[0, 1\])" title="r_1, r_2 \sim U(\[0, 1\])" class="math inline" /> are uniformly sampled, a random piont on the triangle is given by</p>
<p><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdisplaystyle%20p%20%3D%20%281%20-%20%5Csqrt%28r_1%29%29%20A%20%2B%20%5Csqrt%28r_1%29%281-r_2%29%20B%20%2B%20%5Csqrt%28r_1%29%20r_2%20C" alt="p = (1 - \sqrt(r_1)) A + \sqrt(r_1)(1-r_2) B + \sqrt(r_1) r_2 C" title="p = (1 - \sqrt(r_1)) A + \sqrt(r_1)(1-r_2) B + \sqrt(r_1) r_2 C" class="math display" /></p>
<p>Alternatively: farthest point sampling (sample next point to be farthest from all previously sampled points). However, this depends on the notion of distance (on mesh: discrete geodesic distance = path along edges).</p>
<h3 id="geometric-operators">Geometric operators</h3>
<p>how to describe geometry of local observation (i.e. point + its neighborhood)?</p>
<ul>
<li>tangent along a surface</li>
<li>curvature (limiting circle as three points come together)</li>
</ul>
<h5 id="on-discrete-curves">On discrete curves</h5>
<p>On discrete curves: problem that points have no well-defined tangent/normal. One reasonable definition: weighted average of incident edges’ normals, weighted by the edge lengths.</p>
<p><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdisplaystyle%20n_v%20%3D%20%5Cfrac%7B%7Ce_1%7C%20n%5C_%7Be_1%7D%20%2B%20%7Ce_2%7C%20n%5C_%7Be_2%7D%7D%7B%7C%7C%7Ce_1%7Cn%5C_%7Be_1%7D%20%2B%20%7Ce_2%7C%20n%5C_%7Be_2%7D%7C%7C%7D" alt="n_v = \frac{|e_1| n\_{e_1} + |e_2| n\_{e_2}}{|||e_1|n\_{e_1} + |e_2| n\_{e_2}||}" title="n_v = \frac{|e_1| n\_{e_1} + |e_2| n\_{e_2}}{|||e_1|n\_{e_1} + |e_2| n\_{e_2}||}" class="math display" /></p>
<figure>
<img src="discrete-curves-normals.png" alt="discrete-curves-normals.png" /><figcaption aria-hidden="true">discrete-curves-normals.png</figcaption>
</figure>
<h5 id="on-point-clouds">On point clouds</h5>
<p>Estimate normal by approximating the plane tangent to the surface, as least-squares fitting problem.</p>
<p>Find neighborhood around point, then estimate a plane by PCA of this neighborhood. (Watch out: Orientation of normal is ambiguous.)</p>
<h5 id="mesh-laplacian">Mesh Laplacian</h5>
<p>Local descriptor that describes connectivity of nodes/edges and surface geometry.</p>
<p><img src="mesh-laplacian.png" alt="mesh-laplacian.png" /> <img src="mesh-laplacian-2.png" alt="mesh-laplacian-2.png" /></p>
<h3 id="useful-software">Useful Software</h3>
<ul>
<li>Meshlab (for viewing/processing meshes)</li>
<li>OpenMesh (for processing meshes)</li>
<li>CGAL (for computational geometry)</li>
</ul>
<h1 id="shapes-alignment-descriptors-similarity">02 - Shapes: Alignment, Descriptors, Similarity</h1>
<h3 id="d-shape-alignment-registration">3D Shape Alignment (<em>Registration</em>)</h3>
<p>Often needed: e.g. in</p>
<ul>
<li>3D scanning (combine different scanned parts)</li>
<li>SLAM = Simultaneous Localization + Mapping (essentially, robot navigation)</li>
<li>protein structure alignment</li>
</ul>
<p>Given: two shapes <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20A%2C%20B" alt="A, B" title="A, B" class="math inline" /> with overlap; register together by rigid transform s.t. distance is minimized: <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20%5Cmin_T%20%5Cdelta%28A%2C%20T%28B%29%29" alt="\min_T \delta(A, T(B))" title="\min_T \delta(A, T(B))" class="math inline" /> for some distance measure <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20%5Cdelta" alt="\delta" title="\delta" class="math inline" />.</p>
<p>Challenges: find both <em>point correspondences</em> and <em>transformation</em>.</p>
<h4 id="rigid-3d-alignment-procrustes">Rigid 3D Alignment: Procrustes</h4>
<p>Goal: find best alignment, given correspondences <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20%28x_i%29%5C_i%2C%20%28y_i%29%5C_i" alt="(x_i)\_i, (y_i)\_i" title="(x_i)\_i, (y_i)\_i" class="math inline" /> (even for different shapes).</p>
<figure>
<img src="chair-alignment.png" alt="chair-alignment.png" /><figcaption aria-hidden="true">chair-alignment.png</figcaption>
</figure>
<p>Namely, find <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20%28R%2C%20t%29" alt="(R, t)" title="(R, t)" class="math inline" /> that minimizes <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20%5Csum_i%20%7C%7CR%20x_i%20%2B%20t%20-%20y_i%7C%7C%5C_2%5E2" alt="\sum_i ||R x_i + t - y_i||\_2^2" title="\sum_i ||R x_i + t - y_i||\_2^2" class="math inline" />. Solved as <em>orthogonal Procrustes problem</em> (1966).</p>
<p>Assume a coordinate system centered at the mean of the <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20%28x_i%29%5C_i" alt="(x_i)\_i" title="(x_i)\_i" class="math inline" />: then the minimization term becomes</p>
<p><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdisplaystyle%20%5Cmin%5C_%7BR%2C%20t%7D%20%5Csum_i%20%7C%7Ct-y_i%7C%7C%5C_2%5E2%20-%202%5Csum_i%20%5Clangle%20R%20x_i%2C%20y_i%20%5Crangle" alt="\min\_{R, t} \sum_i ||t-y_i||\_2^2 - 2\sum_i \langle R x_i, y_i \rangle" title="\min\_{R, t} \sum_i ||t-y_i||\_2^2 - 2\sum_i \langle R x_i, y_i \rangle" class="math display" /></p>
<p>The first sum is minimized by <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20t%20%3D%20%28%5Csum%20y_i%29%2FN%20%3D%3A%20%5Cbar%7By%7D" alt="t = (\sum y_i)/N =: \bar{y}" title="t = (\sum y_i)/N =: \bar{y}" class="math inline" />. To minimize the second sum, define the mean-centered <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20X%20%3D%20%28x_0%20-%20%5Cbar%7Bx%7D%2C%20%5Cdots%2C%20x_n%20-%20%5Cbar%7Bx%7D%29%5E%5Ctop" alt="X = (x_0 - \bar{x}, \dots, x_n - \bar{x})^\top" title="X = (x_0 - \bar{x}, \dots, x_n - \bar{x})^\top" class="math inline" />, <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20Y%20%3D%20%28y_0%20-%20%5Cbar%7By%7D%2C%20%5Cdots%2C%20y_n%20-%20%5Cbar%7By%7D%29%5E%5Ctop" alt="Y = (y_0 - \bar{y}, \dots, y_n - \bar{y})^\top" title="Y = (y_0 - \bar{y}, \dots, y_n - \bar{y})^\top" class="math inline" /> and compute the SVD of <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20XY%5E%5Ctop" alt="XY^\top" title="XY^\top" class="math inline" />: <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20XY%5E%5Ctop%20%3D%20UDV%5E%5Ctop" alt="XY^\top = UDV^\top" title="XY^\top = UDV^\top" class="math inline" />.</p>
<p>Now replace the diagonal matrix <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20D" alt="D" title="D" class="math inline" /> by <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20S" alt="S" title="S" class="math inline" />: either by <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20S%20%3D%20I" alt="S = I" title="S = I" class="math inline" /> if <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20%5Cdet%28U%29%5Cdet%28V%29%3D1" alt="\det(U)\det(V)=1" title="\det(U)\det(V)=1" class="math inline" />, or <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20S%20%3D%20%5Ctext%7Bdiag%7D%281%2C%20%5Cdots%2C%201%2C%20-1%29" alt="S = \text{diag}(1, \dots, 1, -1)" title="S = \text{diag}(1, \dots, 1, -1)" class="math inline" /> otherwise. Then the minimizer <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20R" alt="R" title="R" class="math inline" /> is given by</p>
<p><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdisplaystyle%20R%20%3D%20USV%5E%5Ctop." alt="R = USV^\top." title="R = USV^\top." class="math display" /></p>
<p>Question: We had problems with this formula in Exercise 1, and had to slightly change it based on the original paper. Is it valid as written here or not?</p>
<h4 id="obtaining-point-correspondences">Obtaining point correspondences</h4>
<h5 id="iterative-closest-points-besl-and-mckay-92">Iterative Closest Points [Besl and McKay ’92]</h5>
<p>Iterative algorithm:</p>
<ul>
<li>assume that the currenty closest points correspond</li>
<li>align these correspondences (Procrustes)</li>
<li>recompute closest points, repeat</li>
</ul>
<p>Converges if the initialization is good enough. Optional steps: weight correspondences (by quality?), reject outlier correspondences before aligning.</p>
<figure>
<img src="iterative-closest-points.png" alt="iterative-closest-points.png" /><figcaption aria-hidden="true">iterative-closest-points.png</figcaption>
</figure>
<p>Runtime: <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20O%28N_A%20%5Ccdot%20N_B%29" alt="O(N_A \cdot N_B)" title="O(N_A \cdot N_B)" class="math inline" /> to find closest points naively, <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20O%28N_A%29" alt="O(N_A)" title="O(N_A)" class="math inline" /> to compute optimal alignment and update. So <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20O%28K%20N_A%20N_B%29" alt="O(K N_A N_B)" title="O(K N_A N_B)" class="math inline" /> overall runtime (where <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20K" alt="K" title="K" class="math inline" /> is the number of iterations, <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20N_A" alt="N_A" title="N_A" class="math inline" /> and <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20N_B" alt="N_B" title="N_B" class="math inline" /> the number of points in shape <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20A" alt="A" title="A" class="math inline" /> and shape <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20B" alt="B" title="B" class="math inline" />).</p>
<p>Better runtime with data structures like kd-trees.</p>
<p>Improved correspondence selection: minimize not pairwise distance, but distance to tangent plane of the surface. This can make the point correspondences more evenly distributed. No closed-form solution anymore, but faster in practice. (No details on how to compute this).</p>
<figure>
<img src="icp-tangent-plane.png" alt="icp-tangent-plane.png" /><figcaption aria-hidden="true">icp-tangent-plane.png</figcaption>
</figure>
<h5 id="global-registration-finding-icp-initialization">Global Registration: Finding ICP Initialization</h5>
<p>General strategy:</p>
<ol type="1">
<li>find a good initialization</li>
<li>refine with ICP</li>
</ol>
<p>Approaches to find an initialization:</p>
<h6 id="exhaustive-search">Exhaustive search</h6>
<p>Just try out “all possible transforms” (or probably, a sufficiently dense subset of all transforms). Of course, this is extremely slow.</p>
<h6 id="normalization-with-pca">Normalization with PCA</h6>
<p>Center shapes, use PCA and align such that the principal directions match up. Works well in some cases, but can also go wrong - problems are:</p>
<ul>
<li>inconsistent orientation of principal directions (e.g. two cars, for one the principal direction points towards the front, for the other towards the back)</li>
<li>unstable axes (e.g. cup with handle =&gt; principal direction is not the expected top-bottom axis through the cup)</li>
<li>partial similarity (chair with back vs. barstool without)</li>
</ul>
<h6 id="random-sampling-ransac">Random sampling (RANSAC)</h6>
<p>RANSAC: pick random pairs of points, estimate alignment (details a bit unclear).</p>
<h6 id="matching-by-invariant-features">Matching by invariant features</h6>
<p>Identify feature points like corners that describe local geometry (“invariant” because they should be invariant under the transformation). Align these feature points.</p>
<figure>
<img src="feature-points.png" alt="feature-points.png" /><figcaption aria-hidden="true">feature-points.png</figcaption>
</figure>
<h3 id="shape-descriptors">Shape Descriptors</h3>
<p>Needed for the feature point matching approach: feature descriptors that can capture the information to answer “are these points similar?”.</p>
<h5 id="spin-images-johnson-and-hebert-99">Spin Images [Johnson and Hebert ’99]</h5>
<p>To describe a point, create a <em>spin image</em> associated with its neighborhood. Neighborhood point contributions are parametrized by a) their distance to the tangent and b) their distance to the normal.</p>
<figure>
<img src="spin-image.png" alt="spin-image.png" /><figcaption aria-hidden="true">spin-image.png</figcaption>
</figure>
<h5 id="point-feature-histograms-rusu-et-al.-09">Point Feature Histograms [Rusu et al. ’09]</h5>
<p>Find neighbors <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20%28q_i%29%5C_i" alt="(q_i)\_i" title="(q_i)\_i" class="math inline" /> of point <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20p" alt="p" title="p" class="math inline" />, compute histograms based on distances, normal, curvature etc.</p>
<h5 id="global-shape-similarity-and-global-shape-descriptors">Global Shape Similarity and Global Shape Descriptors</h5>
<p>Capture models by high-dimensional shape descriptors, compare these descriptors with some similarity measure.</p>
<h6 id="shape-histograms">Shape Histograms</h6>
<p>Histograms that capture how much surface area resides withing concentric shells of different radii.</p>
<p>Can be made a local shape descriptor restricting the shell radii.</p>
<h3 id="non-rigid-shape-matching">Non-Rigid shape matching</h3>
<p>Goal: find correspondences that preserve the <em>geodesic distance</em> on the shapes. In other words: even if the actual shape changes, pathes along the surface of corresponding points should stay the same.</p>
<figure>
<img src="nonrigid-elephant.png" alt="nonrigid-elephant.png" /><figcaption aria-hidden="true">nonrigid-elephant.png</figcaption>
</figure>
<p>One way to compute something like this: <em>near isometries preserve local structure</em>, so use descriptors of local regions and establish mappings between these. A problem: how to choose the scale of a local region?</p>
<h4 id="intrinsic-similarity-measures">Intrinsic similarity measures</h4>
<h5 id="gromov-hausdorff-distance">Gromov-Hausdorff distance</h5>
<p>The <em>Hausdorff distance</em> between two point sets is the <em>maximum of all minimum distances</em> <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20%5Cmax_p%20min%5C_%7Bq%7D%20d%28p%2C%20q%29" alt="\max_p min\_{q} d(p, q)" title="\max_p min\_{q} d(p, q)" class="math inline" />.</p>
<p>The <em>Gromov-Hausdorff distance</em> is the infimum of Hausdorff distances over all mappings/correspondences.</p>
<h5 id="heat-kernel-signature-sun-et-al.-09">Heat kernel signature [Sun et al. ’09]</h5>
<p>Heat kernel <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20k_t%28x%2C%20y%29" alt="k_t(x, y)" title="k_t(x, y)" class="math inline" />: amount of heat transfered from <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20x" alt="x" title="x" class="math inline" /> to <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20y" alt="y" title="y" class="math inline" /> in time <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20t" alt="t" title="t" class="math inline" />. Advantage: invariant under isometric deformations, works at multiple scales. Difficult to use in real-world scenarios with partial/noisy data, though.</p>
<figure>
<img src="heat-kernel.png" alt="heat-kernel.png" /><figcaption aria-hidden="true">heat-kernel.png</figcaption>
</figure>
<h3 id="shape-search">Shape Search</h3>
<p>Find shapes similar to given shape in shape search engine. Approaches: bag of geometric words, i.e. decompose shape into some parts.</p>
<p>Retrieve similar shapes through embedding in descriptor space (also see <a href="#joint-embedding-for-retrieval">Joint Embedding for Retrieval</a> and <a href="#joint-embedding-of-3d-scans-and-cad-objects-dahnert-et-al-19">Joint embedding of 3D scans and CAD objects</a> in Lecture 5).</p>
<h1 id="machine-learning-foundations">03 - Machine Learning Foundations</h1>
<p>(I skipped this one because it’s only a repetition of I2DL/ML topics)</p>
<h1 id="shape-segmentation-and-labeling">04 - Shape Segmentation and Labeling</h1>
<h3 id="d-classification-tasks">3D Classification tasks</h3>
<h5 id="voxnet-maturana-and-scherer-15">VoxNet [Maturana and Scherer ’15]</h5>
<p>NN architecture working on occupancy grids. One of the first 3D CNNs, used for object classification on occupancy grids.</p>
<h5 id="d-cnn-qi-et-al.-16">3D CNN [Qi et al. ’16]</h5>
<p>Also, object classification on occupancy grids. Uses <em>anisotropic kernels</em> and <em>network in network</em> (details?). Interesting: Performs much better on synthetic objects (89.9% acc.) than on real objects (74.5% acc.).</p>
<h5 id="pointnet-qi-et-al.-17">PointNet [Qi et al. ’17]</h5>
<p>Works on a point cloud (unordered set of points). Can then be used for</p>
<ul>
<li>classification</li>
<li>part segmentation</li>
<li>semantic segmentation</li>
</ul>
<p>Architecture:</p>
<ol type="1">
<li>Transform all points by a learned transformation (3x3 matrix; goal: bring into <em>aligned coordinate system</em>)</li>
<li>1x1 1d convolutions from 3 -&gt; 64 channels (basically <em>shared MLP</em>) with ReLU and Batch norm</li>
<li>Another learned transform, 64x64 (close to orthogonal)</li>
<li>1x1 1d convolutions from 64 channels -&gt; 128 channels -&gt; 1024 channels with ReLUs and Batch norms</li>
<li>Max pooling (they argue: important that it’s a symmetric operation, since the input set is unordered)</li>
<li>More MLPs -&gt; final output</li>
</ol>
<figure>
<img src="point-net.png" alt="point-net.png" /><figcaption aria-hidden="true">point-net.png</figcaption>
</figure>
<p>Question: How does this relate to the architecture in Ex. 2? Is it really the same?</p>
<ul>
<li>advantage: more efficient on sparse input data</li>
<li>disadvantage: doesn’t exploit local information</li>
</ul>
<h5 id="pointnet-qi-et-al.-17-1">PointNet++ [Qi et al. ’17]</h5>
<p>Unlike PointNet, also capture local structures (introduce hierarchical processing layer: grouping)</p>
<h3 id="d-shape-segmentation-tasks">3D Shape segmentation tasks</h3>
<p>Motivation: human understand shapes by their parts; can guide other tasks like shape matching, shape recognition, part-based modeling.</p>
<p>Labeling: assign part label to each point (or face) of the shape.</p>
<h4 id="classical-approaches">Classical approaches</h4>
<p>Some (older) approaches work on single shapes: K-Means [Shalfman et al. ’02], Normalized Cuts [Golovinskiy and Funkhouser ’08], Primitive Fitting [Attene et al. ’06], Random Walks [Lai et al. ’08].</p>
<p>Better: learn from whole collection of shapes (consistently segment on a collection of objects)</p>
<h5 id="conditional-random-fields-for-shape-labeling-kalogerakis-et-al.-10">Conditional Random Fields for Shape Labeling [Kalogerakis et al. ’10]</h5>
<p>Conditional Random Fields (CRFs), encoding relationships between neighboring faces (details: see slides).</p>
<p>Some limitations: e.g. all legs are labeled together, the individual legs cannot be distinguished. Also, sensitive to topology.</p>
<h4 id="deep-learning-based-part-segmentation">Deep Learning-based Part Segmentation</h4>
<p>Helpful: ShapeNet dataset (~51k models in <em>ShapeNetCore</em>)</p>
<p>Simple option: 3D CNN for segmentation (convolve, have # classes channels in the end)</p>
<p>More advanced options:</p>
<h5 id="pointnet-for-part-segmentation-qi-et-al.-17">PointNet for part segmentation [Qi et al. ’17]</h5>
<p>Different part segmentation head (concat each of the point features from before the pooling with the global feature obtained from the pooling. Put resulting nx1088 data through shared MLPs to get output scores nxm - n points, m classes)</p>
<figure>
<img src="pointnet-part-segmentation.png" alt="pointnet-part-segmentation.png" /><figcaption aria-hidden="true">pointnet-part-segmentation.png</figcaption>
</figure>
<h5 id="structurenet-mo-et-al.-19">StructureNet [Mo et al. ’19]</h5>
<p>Take into account part hierarchy.</p>
<p>Encode relationships between neighboring segments via a graph that encodes object parts. Use a Graph NN and encoder/decoder structure (<em>graph variational autoencoder</em>).</p>
<p>(details -&gt; slides)</p>
<figure>
<img src="structure-net.png" alt="structure-net.png" /><figcaption aria-hidden="true">structure-net.png</figcaption>
</figure>
<h5 id="excursion-graph-nns">Excursion: Graph NNs</h5>
<h3 id="unsupervised-co-segmentation">Unsupervised Co-Segmentation</h3>
<p>Goal: find <em>most consistent</em> segmentation accross whole collection of shapes, without any supervision signal.</p>
<h5 id="feature-based-approach-sidi-et-al.-11">Feature-based approach [Sidi et al. ’11]</h5>
<p>Segment per object; find similar parts accross objects (clustering) -&gt; end result is a part segmentation of whole collection.</p>
<figure>
<img src="descriptor-space-co-segmentation.png" alt="descriptor-space-co-segmentation.png" /><figcaption aria-hidden="true">descriptor-space-co-segmentation.png</figcaption>
</figure>
<p>Drawback: uses handcrafted features.</p>
<h5 id="bae-net-branched-autoencoder-for-shape-co-segmentation-chen-et-al.-19">BAE-NET: Branched Autoencoder for Shape Co-Segmentation [Chen et al. ’19]</h5>
<p><em>Branched autoencoder</em>. Reconstruct each part of the shape. Intuition: apprarently the autoencoder does not only learn good features for reconstructing the whole thing, but also for reconstructing/segmenting the parts.</p>
<h5 id="adacoseg-weakly-supervised-co-segmentation-zhu-et-al.-20">AdaCoSeg: Weakly Supervised Co-Segmentation [Zhu et al. ’20]</h5>
<p>Train a) on shapes that are segmented (but not necessarily consistently; e.g. accross several chairs, you don’t always have one-to-one corresponding parts/labels) (<em>Part prior network</em>) b) in an unsupervised way on a <em>co-segmentation network</em></p>
<p>Overall architecture complex -&gt; see slides.</p>
<h3 id="active-learning-human-in-the-loop">Active Learning: Human-in-the-loop</h3>
<p>System queries human “oracle”, minimally. For example:</p>
<ul>
<li>verification</li>
<li>producing labels</li>
</ul>
<p>Goal: human should do as little work as possible.</p>
<figure>
<img src="human-in-the-loop.png" alt="human-in-the-loop.png" /><figcaption aria-hidden="true">human-in-the-loop.png</figcaption>
</figure>
<h5 id="active-part-segmentation-learning">Active Part Segmentation Learning</h5>
<ul>
<li>first: human annotates some shapes</li>
<li>then: model propagates the annotations to other shapes</li>
<li>next: human verifies the labelings of the model</li>
</ul>
<h3 id="datasets-for-shape-segmentation">Datasets for Shape Segmentation</h3>
<ul>
<li>PartNet (most useful)</li>
<li>COSEG Dataset</li>
<li>LabelMeshes</li>
<li>Princeton Segmentation Benchmark</li>
</ul>
<h1 id="shape-generation">05 - Shape Generation</h1>
<p>Goal: be able to generate shapes automatically. Usecases: for example, allow amateurs to create quality 3D models, and professionals to reduce repetitive work. Also, complete 3D structures that were partially observed.</p>
<h3 id="example-usecases">Example usecases</h3>
<ul>
<li>Modeling by Example [Funkhouser et al. ’04]: Technique where the user selects a part from a model and part of another model to replace it with, which are then automatically fused. For example, fuse a chair back, chair legs, seating surface from different chair models.</li>
<li>Part suggestions to support creativity [Chaudhuri and Koltun ’10]</li>
<li>Semantic part suggestion [Chaudhuri et al. ’13] (make plane <em>more aerodynamic</em> or animal <em>more scary</em>)</li>
</ul>
<h3 id="shape-reconstruction">Shape Reconstruction</h3>
<h5 id="d-epn-for-shape-completion-dai-et-al.-17">3D-EPN for Shape Completion [Dai et al. ’17]</h5>
<p>Classification of partial shape; class label + input scan -&gt; encoder - predictor network (32^2 voxel grid) -&gt; database prior, multiresolution 3D shape synthesis -&gt; output distance field</p>
<figure>
<img src="3d-epn.png" alt="3d-epn.png" /><figcaption aria-hidden="true">3d-epn.png</figcaption>
</figure>
<h5 id="d-r2n2-3d-reconstruction-from-multi-view-images-choy-et-al.-16">3D-R2N2: 3D Reconstruction from Multi-View Images [Choy et al. ’16]</h5>
<p>Recurrent approach: Encode each image with 2D CNN, then run feature vectors through convolutional LSTM and decode into a 3D occupancy grid with a CNN.</p>
<figure>
<img src="3d-R2N2.png" alt="3d-R2N2.png" /><figcaption aria-hidden="true">3d-R2N2.png</figcaption>
</figure>
<h5 id="deepsdf-implicit-3d-reconstruction-park-et-al.-19">DeepSDF: Implicit 3D Reconstruction [Park et al. ’19]</h5>
<p>Auto-decoder architecture: like auto-encoder without the encoder part. Instead, the decoder works on codes sampled from an artificial latent space, which is optimized jointly with the decoder.</p>
<p>Allows</p>
<ul>
<li>shape completion from single depth image (how?)</li>
<li>shape interpolation via interpolating the corresponding latent codes</li>
</ul>
<h5 id="occupancy-networks-mescheder-et-al.-19">Occupancy Networks [Mescheder et al. ’19]</h5>
<p>Also implicit 3D reconstruction: instead of an SDF, just reconstruct the function “p <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20%5Cmapsto" alt="\mapsto" title="\mapsto" class="math inline" /> p inside the object?” implicitly.</p>
<h3 id="point-cloud-generation">Point Cloud Generation</h3>
<p>“Just predict 512 points” or similar; less resolution &lt;-&gt; space tradeoff problems than with voxels.</p>
<h5 id="psgn-point-set-generation-fan-et-al.-17">PSGN (Point Set Generation) [Fan et al. ’17]</h5>
<p>Input: segmented image. Output: point cloud that represents the segmented object.</p>
<p>Input image -&gt; 2D CNN -&gt; MLP generates points.</p>
<ul>
<li>Chamfer loss: for each point in target point set, distance to closest point in prediction point set; and vice versa; summed together
<ul>
<li>both directions, since else the loss could be “cheated” by predicting a sub- or superset</li>
</ul></li>
</ul>
<figure>
<img src="psgn.png" alt="psgn.png" /><figcaption aria-hidden="true">psgn.png</figcaption>
</figure>
<h3 id="parametric-3d-model-generation-smirnov-et-al.-21">Parametric 3D model generation [Smirnov et al. ’21]</h3>
<p>Sketch-based task: from a 2D sketch, generate 3D shape.</p>
<p>Reconstruction via <em>Coon’s patch</em>:</p>
<ul>
<li>parametric representation of a surface in computer graphs (smoothly joins surfaces together)</li>
<li>specified: four curves that bound the patch
<ul>
<li><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20P%28%5Cmu%2C%200%29%2C%20P%28%5Cmu%2C%201%29%2C%20P%280%2C%20%5Cnu%29%2C%20P%281%2C%20%5Cnu%29" alt="P(\mu, 0), P(\mu, 1), P(0, \nu), P(1, \nu)" title="P(\mu, 0), P(\mu, 1), P(0, \nu), P(1, \nu)" class="math inline" /></li>
</ul></li>
<li>Linearly interpolate between these curves: <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdisplaystyle%20P%28%5Cmu%2C%20%5Cnu%29%20%3D%20P%28%5Cmu%2C%200%29%281%20-%20%5Cnu%29%20%2B%20P%28%5Cmu%2C%201%29%5Cnu%20%2B%20P%280%2C%20%5Cnu%29%281-%5Cmu%29%20%2B%20P%281%2C%20%5Cnu%29%5Cmu%20-%20P%280%2C%200%29%281%20-%20%5Cmu%29%281-%5Cnu%29%20-%20P%280%2C%201%29%281-%5Cmu%29%5Cnu%20-%20P%281%2C0%29%5Cmu%281-%5Cnu%29%20-%20P%281%2C1%29%5Cmu%5Cnu" alt="P(\mu, \nu) = P(\mu, 0)(1 - \nu) + P(\mu, 1)\nu + P(0, \nu)(1-\mu) + P(1, \nu)\mu - P(0, 0)(1 - \mu)(1-\nu) - P(0, 1)(1-\mu)\nu - P(1,0)\mu(1-\nu) - P(1,1)\mu\nu" title="P(\mu, \nu) = P(\mu, 0)(1 - \nu) + P(\mu, 1)\nu + P(0, \nu)(1-\mu) + P(1, \nu)\mu - P(0, 0)(1 - \mu)(1-\nu) - P(0, 1)(1-\mu)\nu - P(1,0)\mu(1-\nu) - P(1,1)\mu\nu" class="math display" /></li>
</ul>
<p>Per category, templates of part decomposition are generated (details?). For each category, a generator is trained:</p>
<ul>
<li>Resnet encodes sketch</li>
<li>FC layer predicts control points</li>
</ul>
<p>Trained with Chamfer distance loss + additional losses (<em>normal alignment</em>, <em>collission penalization</em>, <em>patch flatness regularizer</em>)</p>
<figure>
<img src="parametric-3d-models.png" alt="parametric-3d-models.png" /><figcaption aria-hidden="true">parametric-3d-models.png</figcaption>
</figure>
<h3 id="reconstructing-explicit-3d-meshes">Reconstructing Explicit 3D Meshes</h3>
<p>(indirectly possible via previous methods, e.g. predict sdf -&gt; apply marching cubes)</p>
<p>Directly: can have loss actually on the mesh. Also, possibly more efficient mesh output than from marching cubes.</p>
<h5 id="pixel2mesh-deforming-template-mesh-wang-et-al.-18">Pixel2Mesh: Deforming template mesh [Wang et al. ’18]</h5>
<p>Start with ellipsoid mesh; deform to e.g. airplane</p>
<p>Architecture: Two pipelines, one convolving the input image, another deforming the mesh.</p>
<ul>
<li>Image undergoes several convolutions</li>
<li>Graph NN predicts vertex displacements based on features from CNN pipeline</li>
</ul>
<figure>
<img src="pixel2mesh.png" alt="pixel2mesh.png" /><figcaption aria-hidden="true">pixel2mesh.png</figcaption>
</figure>
<p>Disadvantage: no different topology possible</p>
<h5 id="mesh-r-cnn-gkioxari-et-al.-19">Mesh R-CNN [Gkioxari et al. ’19]</h5>
<ul>
<li>Start with coarse occupancy grid obtained from image -&gt; create template mesh from it -&gt; refine with deformation approach.</li>
</ul>
<p>(not many details)</p>
<h5 id="freeform-mesh-generation-scan2mesh-dai-and-niessner-19">Freeform Mesh Generation: Scan2Mesh [Dai and Niessner ’19]</h5>
<p>“Cut out the template intermediary” Details: see <a href="#scan2mesh-dai-et-al-19">here in Lecture 6</a>.</p>
<h5 id="retrieval-based-object-representation-li-et-al.-15">Retrieval-based Object Representation [Li et al. ’15]</h5>
<p>Retrieve a similar looking object’s mesh from a database (say, ShapeNet). Enables real-time 3D reconstruction!</p>
<ul>
<li>Retrieving similar objects: matching constellations of keypoints and descriptors.</li>
</ul>
<p>Joint Embedding Space: space of both real images and shapes, s.t. semantically similar things are close - constructed from multi-view features - images mapped into space via CNN</p>
<h3 id="joint-embedding-for-retrieval">Joint Embedding for Retrieval</h3>
<p>…means that shapes and images are embedded in a joint embedding space (<em>CNN image purification</em>). Used in the <a href="#retrieval-based-object-representation-li-et-al-15">previous method</a>.</p>
<h5 id="joint-embedding-of-shapesimages-li-et-al.-15">Joint embedding of shapes/images [Li et al. ’15]</h5>
<ul>
<li>shapes: construct embedding space based on multi-view features</li>
<li>images: train CNN to learn to map images into the embedding space</li>
</ul>
<figure>
<img src="joint-embedding.png" alt="joint-embedding.png" /><figcaption aria-hidden="true">joint-embedding.png</figcaption>
</figure>
<h5 id="joint-embedding-of-3d-scans-and-cad-objects-dahnert-et-al.-19">Joint embedding of 3D scans and CAD objects [Dahnert et al. ’19]</h5>
<p>Construct embedding space end-to-end; use triplet loss for metric learning. This means that an instance is compared with a known positive correspondence (should be close) and a known negative correspondence (should be far away).</p>
<h5 id="mask2cad-kuo-et-al.-20">Mask2CAD [Kuo et al. ’20]</h5>
<p>Start with image input. Segment into instances -&gt; calculate embedding -&gt; retrieve shape with close embeding. Also, classify the object pose. Combine retrieved shapes with refined poses into reconstruction.</p>
<figure>
<img src="mask2cad.png" alt="mask2cad.png" /><figcaption aria-hidden="true">mask2cad.png</figcaption>
</figure>
<h1 id="learning-on-different-3d-representations">06 - Learning on Different 3D Representations</h1>
<h3 id="types-of-3d-represenations">Types of 3D Represenations</h3>
<ol type="1">
<li>Volumetric grids =&gt; 3D CNNs</li>
<li>Multi-view</li>
<li>Point clouds</li>
<li>Meshes</li>
</ol>
<h3 id="volumetric-grids">Volumetric Grids</h3>
<p>More efficient than dense grid? =&gt; octtree</p>
<h5 id="octnet-operating-on-octtree-structure">OctNet: Operating on Octtree structure</h5>
<p>Problem: “neighbor queries” needed for convolutions can be less efficient Needed: known octtree structure =&gt; no generative tasks</p>
<p>Other architectures support generative tasks:</p>
<h5 id="octtree-generating-networks">Octtree Generating Networks</h5>
<p>Predict if octtree cell is empty / filled / mixed</p>
<h4 id="sparse-techniques">Sparse Techniques</h4>
<p>“Representing ‘the fine level’ in a sparse fashion” e.g. storing only the surface voxels with hashtable</p>
<h5 id="submanifold-sparse-convolutions">Submanifold sparse convolutions</h5>
<p>Avoid spreading sparse information to dense via convolution <img src="submanifold-sparse.png" alt="submanifold-sparse.png" /> Submanifold sparse convolutions operate basically only close to the surface (details?)</p>
<ul>
<li>memory-efficient; uses hashing</li>
<li>significant performance improvement (e.g. room-scale <span class="citation" data-cites="1-2cm">@1-2cm</span> now possible)</li>
<li>See: <code>MinkowskiImage</code></li>
</ul>
<p>(Slight) disadvantage: close points in euclidean space can be far removed in geodesic distance or even disconnected, in this case no information propagation by subman. sparse conv. is possible.</p>
<h5 id="sparse-generative-nns-sg-nn-dai-et-al.-20">Sparse Generative NNs (SG-NN, [Dai et al. ’20])</h5>
<p>(Used for scan completion)</p>
<p>Sparse Encoder -&gt; Coarse Resolution Dense Predictor @ Coarse Prediction Upsample + Filter out by Geometry (e.g. mask out space known to be empty)</p>
<h3 id="multi-view-representations">Multi-View Representations</h3>
<p>Idea:</p>
<ul>
<li>exploit already existing powerful 2D CNN work</li>
<li>leverage high resolution of images</li>
</ul>
<p>Advantage: can use fine-tuning on huge available 2D image datasets (which are not there for 3D)</p>
<p>Put in multiple images from the same object, put them through CNN. Idea: maybe one view is surer than another, and can provide more information.</p>
<p>Actually outperforms approaches with 3D inputs because of more training data.</p>
<figure>
<img src="mvcnn.png" alt="mvcnn.png" /><figcaption aria-hidden="true">mvcnn.png</figcaption>
</figure>
<p>Another idea: use on point clouds by rendering point clouds as spheres</p>
<h3 id="handling-irregular-structures">Handling Irregular Structures</h3>
<p>Meshes, Point Sets, Molecule Graphs, …</p>
<p>Surface geometry: up to now, gridify 2d surface in 3d space and apply (possibly sparse) convolution. =&gt; Point Cloud represents this without grid</p>
<h5 id="point-clouds-1">Point Clouds</h5>
<p>e.g. <a href="">PointNet</a>: pool over points, such that agnostic to ordering</p>
<h3 id="meshes">Meshes</h3>
<p>Interpret as graphs!</p>
<p>Convolutions over graphs should have, like regular convolutions: local support, weight sharing; and they should enable analysis at varying scales.</p>
<h5 id="geometric-operators-1">Geometric operators</h5>
<p>Some geometric operators that might be useful for convolutions (how exactly? After all we’re not on a continuous surface?)</p>
<ul>
<li>tangent plane of a point <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20x" alt="x" title="x" class="math inline" /></li>
<li>geodesic distance between two points <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20x%2C%20x%27" alt="x, x&#39;" title="x, x&#39;" class="math inline" /></li>
<li>gradient <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20%5Cnabla%20f%28x%29" alt="\nabla f(x)" title="\nabla f(x)" class="math inline" /> of a scalar field <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20f" alt="f" title="f" class="math inline" /> on the surface</li>
<li>divergence <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20%5Ctext%7Bdiv%7D%20F%28x%29" alt="\text{div} F(x)" title="\text{div} F(x)" class="math inline" /> of a vector field <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20F" alt="F" title="F" class="math inline" /> on the surface (density of outward flux of <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20F" alt="F" title="F" class="math inline" /> from an infinitesimal volume around <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20f" alt="f" title="f" class="math inline" />)</li>
<li>Laplacian: <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20%5CDelta%20f%28x%29%20%3D%20-%5Ctext%7Bdiv%7D%28%5Cnabla%20f%28x%29%29" alt="\Delta f(x) = -\text{div}(\nabla f(x))" title="\Delta f(x) = -\text{div}(\nabla f(x))" class="math inline" /> (difference between <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20f%28x%29" alt="f(x)" title="f(x)" class="math inline" /> and the average of <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20f%28x%29" alt="f(x)" title="f(x)" class="math inline" /> on an infinitesimal sphere around <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20x" alt="x" title="x" class="math inline" />)</li>
<li>discrete Laplacian (on a grid structure)
<ul>
<li>in 1D: <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20%28%5CDelta%20f%29%2Ai%20%5Capprox%202f_i%20-%20f%2A%7Bi-1%7D%20-%20f%5C_%7Bi%2B1%7D" alt="(\Delta f)*i \approx 2f_i - f*{i-1} - f\_{i+1}" title="(\Delta f)*i \approx 2f_i - f*{i-1} - f\_{i+1}" class="math inline" /></li>
<li>in 2D: <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20%28%5CDelta%20f%29%2A%7Bi%2Cj%7D%20%5Capprox%204f%2A%7Bij%7D%20-%20f%5C_%7Bi-1%2Cj%7D%20-%20f%5C_%7Bi%2B1%2Cj%7D%20-%20f%5C_%7Bi%2Cj-1%7D%20-%20f%5C_%7Bi%2Cj%2B1%7D" alt="(\Delta f)*{i,j} \approx 4f*{ij} - f\_{i-1,j} - f\_{i+1,j} - f\_{i,j-1} - f\_{i,j+1}" title="(\Delta f)*{i,j} \approx 4f*{ij} - f\_{i-1,j} - f\_{i+1,j} - f\_{i,j-1} - f\_{i,j+1}" class="math inline" /></li>
</ul></li>
</ul>
<p>Similar definitions of discrete Laplacian on undirected graphs/triangular meshes (formulas -&gt; see slides). Related to this: Graph Fourier transform (no more details on this).</p>
<h5 id="geodesic-cnn">Geodesic CNN</h5>
<p>Convolutions in local geodesic coordinate system <img src="geodesic.png" alt="geodesic.png" /> (Polar coordinates <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20%5Crho%2C%20%5Ctheta" alt="\rho, \theta" title="\rho, \theta" class="math inline" />)</p>
<p>Apply filters to geodesic patches (to be rotation-invariant: apply several rotated variants of the filter. Otherwise the choice of <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20%5Ctheta%3D0" alt="\theta=0" title="\theta=0" class="math inline" /> is arbitrary)</p>
<figure>
<img src="geodesic-CNN.png" alt="350" /><figcaption aria-hidden="true">350</figcaption>
</figure>
<p>Then one can define Convolutional networks with these geodesic convolutions [Masci et al. ’15]</p>
<figure>
<img src="geodesic-CNN-full-network.png" alt="geodesic-CNN-full-network.png" /><figcaption aria-hidden="true">geodesic-CNN-full-network.png</figcaption>
</figure>
<p>Computing this in practice: not trivial, but possible: uses a marching-like procedure that needs a triangular mesh. One needs to choose the radius of geodesic patches (compare with the kernel size).</p>
<p>Drawback: There is no natural pooling operation; can only use convolutions to increase receptive field. This is a huge drawback and hinders the performance.</p>
<h5 id="spectral-graph-convolutions">Spectral Graph Convolutions</h5>
<p>Relies on Graph fourier analysis. Convolution thereom: convolution in spatial domain = multiplication in frequency space.</p>
<p>Disadvantages: unless you have “perfect data”, there are some drawbacks; e.g.g no guarantee that kernels have local support, no shift invariance. Also no natural pooling operations.</p>
<p>Open area for new developments.</p>
<h5 id="meshcnn-hanocka-et-al.-19">MeshCNN [Hanocka et al. ’19]</h5>
<p>CNN for triangle meshes: conv. and pooling defined on edge</p>
<ul>
<li>each edge has a feature, and four edge neighbors (from two incident faces)</li>
<li>convolution applied to edge feature + 4 neighbors</li>
<li>pooling: edge collapse</li>
</ul>
<figure>
<img src="MeshCNN.png" alt="MeshCNN.png" /><figcaption aria-hidden="true">MeshCNN.png</figcaption>
</figure>
<p>Can perform reasonably well for part segmentation.</p>
<h4 id="message-passing-graph-nns">Message Passing Graph NNs</h4>
<p>Nodes in a graph have features (<em>hidden states</em>) - optionally, also the edges and the graph as a whole. Hidden states are updated by aggregating messages from neighboring vertices/edges.</p>
<h5 id="scan2mesh-dai-et-al.-19">Scan2Mesh [Dai et al. ’19]</h5>
<p>Constructs a mesh from a 3D scan.</p>
<p>3d Input scan -&gt; downsample (convolutions), predict vertices -&gt; predict edges (message passing network) -&gt; dual graph (where each vertex corresponds to a mesh face, and an edge means that two faces are adjacent) -&gt; output mesh. (see slides)</p>
<figure>
<img src="scan2mesh.png" alt="scan2mesh.png" /><figcaption aria-hidden="true">scan2mesh.png</figcaption>
</figure>
<p>Difficulty: how to define loss? combine: be close to a ground-truth mesh (not very flexible), be close to the correct surface (can have weird artifacts in mesh: e.g. self intersections)</p>
<h3 id="combining-representations">Combining Representations</h3>
<p>It can be an advantage to use multiple representations at the same time.</p>
<h5 id="dynamic-graph-cnn-for-point-clouds">Dynamic Graph CNN for Point Clouds</h5>
<p>From point cloud, compute local neighborhood graph (<img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20k" alt="k" title="k" class="math inline" />-nn graph)</p>
<ul>
<li>apply graph convolution</li>
<li>re-compute local neighborhood graph for the pointcloud points</li>
</ul>
<h5 id="dmv-joint-3d-multi-view-learning-dai-et-al.-18">3DMV (Joint 3D-Multi-View Learning) [Dai et al. ’18]</h5>
<p>First use a 2d CNN on images, then backproject to 3d, then a 3d CNN on reconstructed geometry (and also the actual geometry)</p>
<h5 id="virtual-multi-view-fusion-kundu-et-al.-20">Virtual Multi-View Fusion [Kundu et al. ’20]</h5>
<p>Instead of real views, use rendered views. Also see <a href="#virtual-mvfusion-kundu-et-al-20">here</a></p>
<h5 id="texturenet-huang-et-al.-19">TextureNet [Huang et al. ’19]</h5>
<p>Semantic labeling of textured meshes. Can leverage the texture signal as well.</p>
<h1 id="semantic-scene-segmentation">07 - Semantic Scene Segmentation</h1>
<p>3D Semantic segmentation: segment scene (e.g. room) in semantic parts (e.g. for each part: to which furniture type it belongs)</p>
<h3 id="popular-benchmarks">Popular Benchmarks</h3>
<ul>
<li>ScanNet benchmark (furniture scenes)</li>
<li>KITTI / KITTI-360 (outdoor, captured on roads)</li>
</ul>
<h3 id="scene-segmentation-vs.-part-segmentation">Scene segmentation vs. Part segmentation</h3>
<p><em>Scene</em> segmentation and <em>part</em> semgentation are similar tasks. Differences:</p>
<ul>
<li>in part segmentation, different types of objects have fixed sets of parts (i.e. a chair has legs, the seating surface, armrests, the back of the chair). Different types and are not considered/trained together, but separately.</li>
<li>scale: small objects in part segmentation, larger rooms/building complexes in scene segmentation</li>
</ul>
<h3 id="d-inputs-vs-2d-inputs">3D inputs vs 2D inputs</h3>
<ul>
<li>obvious: cubic vs quadratic space growth</li>
<li>less obvious: in 3D, you get information about scale!
<ul>
<li>in 2D you don’t: you see a projection. One pixel can correspond to different real-world lengths at different positions in a picture.</li>
<li>but on the other hand, resizing 3D shapes changes this scale information</li>
</ul></li>
</ul>
<h3 id="solving-size-problem">Solving size problem</h3>
<p>You often cannot process a huge scene at full resolution at once without resizing! Compare with <a href="">PointNet, PointNet++</a>: combine local and global information, do things chunk by chunk.</p>
<h5 id="sliding-window-approach-dai-et-al.-17">Sliding window approach [Dai et al. ’17]</h5>
<p>For semantic segmentation.</p>
<ul>
<li>predict column-by-column in gravity direction (height-reliable feature)</li>
<li>use data of local neighborhood around currently considered column</li>
</ul>
<h5 id="joint-3d-multi-view-dai-et-al.-18">Joint 3d + multi-view [Dai et al. ’18]</h5>
<p>Multi-view images have higher resolution and also color information!</p>
<p>Process multiview: first process in 2d, then backproject to 3d using known camera params. Feed results + original 3d geometry into 3d convolutions.</p>
<h5 id="scancomplete-dai-et-al.-18">ScanComplete [Dai et al. ‘18’]</h5>
<p>Convolutional architecture (no details given), key point: predicting complete geometry helps with semantic segmentation. <img src="scan-complete.png" alt="scan-complete.png" /></p>
<h5 id="sliding-window-approaches-in-general">Sliding window approaches in general</h5>
<p>Adaptive to varying scene sizes. Drawback: for w x h x d scene, needs to be run O(w x h) times.</p>
<h3 id="convolutions-to-allow-varying-input-sizes">Convolutions to allow varying input sizes</h3>
<p>Convolutions can have parameters s.t. the output size = input size.</p>
<h5 id="scancomplete-dai-et-al-18-for-geometric-completion.">ScanComplete [Dai et al ’18] for geometric completion.</h5>
<p>Relevant to semantic segmentation: first <em>completing</em> the scene helped later to correctly <em>segment</em> it.</p>
<h3 id="allowing-higher-resolution">Allowing higher resolution</h3>
<h5 id="texturenet-huang-et-al.-19-1">TextureNet [Huang et al. ’19]</h5>
<p>Augment input with textures (I think? Not clear what happens exactly…)</p>
<p>Textures give higher-resolution input signal.</p>
<h5 id="sparse-convolutions">Sparse convolutions</h5>
<p><a href="#submanifold-sparse-convolutions">Seen before:</a> convolve on active sites only. Improved semantic segmentation drastically (e.g. SparseConvNet, MinkowskiNet; 2019). Mostly because whole scenes can be processed at once now without running out of memory.</p>
<h3 id="non-regular-grid-geometry">Non-Regular Grid Geometry</h3>
<p>What about inputs that don’t natively lie in a 3d grid?</p>
<h5 id="kpconv">KPConv</h5>
<p>Based on point inputs; Goal: create <em>adaptive</em> convolution kernel (“deformable”?) by learning where points should be shifted (no details)</p>
<figure>
<img src="kpconv.png" alt="kpconv.png" /><figcaption aria-hidden="true">kpconv.png</figcaption>
</figure>
<h3 id="online-semantic-segmentation">Online Semantic Segmentation</h3>
<h5 id="occuseg-han-et-al.-20">OccuSeg [Han et al. 20]</h5>
<p>Can perform semantic segmentation in real time + online when walking around with a camera.</p>
<ul>
<li>pretty complex approach</li>
<li>3D UNet followed by more stuff; also, clustering to super-voxels; then graph NN</li>
<li>Most important part apparently is the clustering into super voxels</li>
</ul>
<figure>
<img src="OccuSeg.png" alt="OccuSeg.png" /><figcaption aria-hidden="true">OccuSeg.png</figcaption>
</figure>
<h3 id="multi-view3d-geometry-fusion-revisited">Multi-View/3D Geometry Fusion Revisited</h3>
<p>Goal: Fuse multi-view images with 3D geometry; get more information into the 3D geometry.</p>
<p>Problems with simply backprojecting real-world images:</p>
<ul>
<li>limited set of views</li>
<li>camera estimation, motion blur etc lead to inaccuracies</li>
<li>colors may be view-dependent</li>
</ul>
<h5 id="virtual-mvfusion-kundu-et-al.-20">Virtual MVFusion [Kundu et al. ’20]</h5>
<p>Generate Synthetic images from 3d scene. Lose real-world information, but also do away with inaccuracies in real-world image capture: more images, wider field of view, ensured to be consistent.</p>
<p>Fusing process: first, apply pretrained 2d semantic segmentation. Then back-project to 3d, aggregate projected 2d features for one 3d point by averaging.</p>
<p>(unclear to me: How to generaate synthetic images that are better than the original (limited) images?)</p>
<h5 id="bpnet">BPNet</h5>
<p><em>Bidirectional</em> interactions between 3d and 2d representations (“2d can inform 3d, but 3d can also inform 2d”). One 2D UNet which works with images and one 3D UNet which works on the reconstructed 3D scene, they interchange information and in the end predict 2D/3D labels.</p>
<figure>
<img src="BPNet.png" alt="BPNet.png" /><figcaption aria-hidden="true">BPNet.png</figcaption>
</figure>
<p>Both the 2D results and the 3D results (slightly) improve because of this bidirectional information sharing.</p>
<h1 id="object-detection-instance-segmentation">08 - Object Detection + Instance Segmentation</h1>
<p>Idea now: Not only detect object classes, but learn to actually distinguish different objects (the chair on the left, the chair on the right, etc.). So high-level goal: “Understand object-ness”.</p>
<p>Applications: virtual furniture rearrangement; robots grabbing objects; both need to know as a first step <em>what the individual objects are</em>.</p>
<h3 id="understanding-object-ness">Understanding Object-ness</h3>
<ol type="1">
<li>Semantic segmentation</li>
<li>Object detection (Bounding boxes)</li>
<li>Instance segmentation (Geometry -&gt; Instance mapping)</li>
</ol>
<figure>
<img src="instance-segmentation.png" alt="instance-segmentation.png" /><figcaption aria-hidden="true">instance-segmentation.png</figcaption>
</figure>
<h3 id="d-object-detection-bounding-boxes">3D Object Detection (Bounding Boxes)</h3>
<p>Bounding boxes usually work well (e.g. furniture objects). It stops to work well for more flexible things like computer cables.</p>
<h5 id="warm-up-mask-r-cnn-for-2d-object-detection">Warm-up: Mask R-CNN for <em>2D</em> Object detection</h5>
<p>Let’s see what works well in 2D first.</p>
<p>Prespecified anchor boxes, find out for each: how likely is it that this covers an object =&gt; proposed bounding boxes</p>
<figure>
<img src="mask-r-cnn.jpg" alt="mask-r-cnn.jpg" /><figcaption aria-hidden="true">mask-r-cnn.jpg</figcaption>
</figure>
<p>Propose bounding boxes -&gt; Refine bounding boxes -&gt; generate object masks.</p>
<h5 id="adapting-to-3d">Adapting to 3D</h5>
<ul>
<li>more anchor boxes?</li>
<li>leverage scale information?</li>
<li>(+) more spatial separation between objects</li>
<li>(-) difficult to capture high resolution</li>
</ul>
<h3 id="first-approaches-exploit-spatial-separation">First Approaches: exploit spatial separation</h3>
<p>Example [Liu et al. ’19] : U-Net style first, semantic segmentation, finally clustering. Clustering assumes good spatial separation.</p>
<figure>
<img src="chair-semantic-segmentation.png" alt="chair-semantic-segmentation.png" /><figcaption aria-hidden="true">chair-semantic-segmentation.png</figcaption>
</figure>
<p>Not working well if objects are close together (multiple chairs = one chair). Inverse problem as well: 2 parts of the same chair but with one part inbetween missing from the scan =&gt; 2 different clusters.</p>
<h3 id="d-sis-semantic-instance-segmentation-hou-et-al.-19">3D-SIS (Semantic Instance Segmentation) [Hou et al. ’19]</h3>
<p>Also uses anchors like Mask R-CNN, predicts masks.</p>
<p>Architecture starts with 2D convs, backproj. to 3D and works with 3D Convs; predict anchors and predicts if they correspond to an object, then refines; finally predicts per voxel which mask it belongs to. (complicated architecture; don’t get all the details)</p>
<p>Trick: train on smaller chunks (while predicting whole scenes in the end).</p>
<h3 id="revealnet-hou-et-al.-20">RevealNet [Hou et al. ’20]</h3>
<p>Hallucinate missing geometry patterns (“to get better priors”), in order to improve the instance segmentation performance.</p>
<p>Architecture: Encoder-Decoder structure (for details see slides).</p>
<figure>
<img src="reveal-net.png" alt="reveal-net.png" /><figcaption aria-hidden="true">reveal-net.png</figcaption>
</figure>
<h3 id="summary-top-down-anchor-based-mask-r-cnn-style-approaches">Summary: Top-down, anchor-based (Mask-R-CNN-style) approaches</h3>
<h5 id="challenges">Challenges</h5>
<ul>
<li>Anchor basis needs to be diverse enough to cover all possibilities</li>
<li>think/“anisotropic” objects easily missed</li>
<li>inefficient to predict objectness for many empty space locations (much more of a problem in 3d than 2d)</li>
</ul>
<h5 id="top-down-vs.-bottom-up">Top-Down vs. Bottom-up</h5>
<p>The simple Clustering approach is Bottom-up. Top down would be e.g. starting with finding an object, then refine it to really get the object</p>
<p>Spectrum between these two paradigms: E.g. the next model, VoteNet, combines some of both.</p>
<h3 id="point-cloud-object-detection">Point Cloud Object detection</h3>
<p>Anchors on point clouds: problematic; since points are often on the surface, don’t coincide with object center. Instead if anchors not restricted to point cloud (what does that mean?) =&gt; not clear how to distribute them</p>
<h5 id="votenet-qi-et-al.-19">VoteNet [Qi et al. ’19]</h5>
<p>Goal: get <em>object detection</em> without scanning too much empty space.</p>
<p>Point cloud representation: get votes from each point to where the center of their object is.</p>
<p>Related to Hough transform (which is usually used to find lines). Instead of lines, we find centers.</p>
<ol type="1">
<li>Input: point cloud. Put through “backbone” (typically PointNet++)</li>
<li>Seeds (typically farthest-point sampling from PointNet++): locations + features learned</li>
<li>Vote (shared MLP): predict 3D location (of center) + feature offset (used together with original feature to compute object feature)</li>
<li>Cluster votes: farthest point sampling based on 3D locations</li>
<li>Shared PointNet processes vote cluster (predict objectness, bounding box params, class category)</li>
<li>“3D non-max suppression”: filter out near-duplicate bounding boxes</li>
</ol>
<figure>
<img src="VoteNet.png" alt="VoteNet.png" /><figcaption aria-hidden="true">VoteNet.png</figcaption>
</figure>
<p>This is a leading conceptual approach for detecting objects.</p>
<h5 id="d-mpa-engelmann-et-al.-20">3D-MPA [Engelmann et al. ’20]</h5>
<p>Multi-proposal Aggregation for 3D Semantic Instance Segmentation. Similar to VoteNet, but now also get <em>instance segmentation</em> out. Works on Point cloud (or here, rather sparse voxels?)</p>
<ol type="1">
<li>Proposal generation (propose features for seed locations)</li>
<li>Proposal consolidation (graph NN with seeds as nodes =&gt; refined proposal features)</li>
<li>Object generation</li>
<li>again, non-max suppression postprocesssing</li>
</ol>
<figure>
<img src="3d-mpa.png" alt="3d-mpa.png" /><figcaption aria-hidden="true">3d-mpa.png</figcaption>
</figure>
<h5 id="probabilistic-embeddings-for-point-cloud-instance-segmentation-zhang-and-wonka-21">Probabilistic Embeddings for Point Cloud Instance Segmentation [Zhang and Wonka ’21]</h5>
<p>(for point cloud instance segmentation)</p>
<p>Bottom-up instance segmentation on point clouds. Input unordered point set <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20x_i" alt="x_i" title="x_i" class="math inline" />; Output per-point embeddings <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20e_i" alt="e_i" title="e_i" class="math inline" />.</p>
<p>Measure similarities between point embeddings, and group the similar points together.</p>
<p>Not clear what happens exactly, but points are encoded as Gaussian distributions, and distances between the distributions are calculated (Bhattacharayya kernel, etc.)</p>
<p>Loss: BCE (nice to optimize; can suffer from class imbalance, e.g. way more background than interesting objects).</p>
<h3 id="towards-part-based-scene-understanding">Towards Part-Based Scene Understanding</h3>
<p>Segment scene not only into instances, but even further segment instances into parts.</p>
<h5 id="bokhovkin-et-al.-21">[Bokhovkin et al. ’21]</h5>
<p>Start with synthetic priors of what e.g. chairs/tables/beds look like and how they decompose into parts. Combine linear combination of priors for a class to get a coarse estimate of how objects of this class look.</p>
<ol type="1">
<li>VoteNet-Like object detection</li>
<li>Decomposing objects into part trees =&gt; latent vector for each part</li>
<li>Prior decoder (recall: shape should be linear combination of part priors)</li>
<li>Refine priors =&gt; complete part decomposition</li>
</ol>
<figure>
<img src="parts-of-tables.png" alt="parts-of-tables.png" /><figcaption aria-hidden="true">parts-of-tables.png</figcaption>
</figure>
<h1 id="reconstructing-and-generating-scenes">09 - Reconstructing and Generating Scenes</h1>
<p>Motivation: make shape generation more accessible, require less expertise</p>
<ul>
<li><p>capturing 3d photos (e.g. can change viewpoint a bit)</p></li>
<li><p>3d online interaction mimicking live interaction (“mixed reality”)</p></li>
<li><p>reconstructing with a Kinect</p>
<ul>
<li>one problem: only partial geometry (<em>missing geometry</em>)
<ul>
<li>maybe because the camera didn’t go there</li>
<li>or objects are occluded</li>
<li>sometimes even from metallic/reflecting surfaces</li>
<li>=&gt; need to complete the scans</li>
</ul></li>
</ul></li>
</ul>
<h3 id="generative-tasks">Generative Tasks</h3>
<h5 id="scan-completion">Scan completion</h5>
<p>Fill in missing geometry in an incomplete scan.</p>
<h5 id="surface-reconstruction">Surface reconstruction</h5>
<p>we have a room (in this particular case, pretty much completely observed); but only point measurements (point cloud). Go from point measurements to whole continuous surfaces (e.g. implicit representation)</p>
<p>This could be done by classical surface reconstruction algorithms, but our goal is to learn geometric priors to help the task (to still perform well e.g. if the measurements are more sparse).</p>
<h5 id="pure-generative-task-scene-generation">Pure generative task: scene generation</h5>
<p>Sample code from latent space, then construct a plausible scene from it.</p>
<figure>
<img src="scene%20generation.png" alt="scene generation.png" /><figcaption aria-hidden="true">scene generation.png</figcaption>
</figure>
<h3 id="excursion-generative-tasks-in-2d">Excursion: Generative tasks in 2D</h3>
<p>Let’s see how similar tasks are tackled in 2D.</p>
<ul>
<li>Encoder-Decoder approaches (encode -&gt; decode -&gt; L1/L2 reconstruction loss; problem with this loss: blurry generations)</li>
<li>GANs (Generator generates; discriminator must tell real/fake apart)
<ul>
<li>both should train “at a similar pace”</li>
</ul></li>
<li>Autoregressive models
<ul>
<li>generate not the whole image at once, but pixel-by-pixel (i.e. when you generate the i-th pixel, you already know the i-1 previous ones)</li>
<li>examples: PixelRNN, PixelCNN, VQ-VAE, VQ-VAE2</li>
</ul></li>
</ul>
<h3 id="scan-completionsurface-reconstruction-tasks-both-appear-in-this-section">Scan completion/Surface reconstruction tasks (both appear in this section)</h3>
<h5 id="synthetic-vs.-true-data">Synthetic vs. true data</h5>
<p>Synthetic data: perfect ground-truth. Synthetic datasets:</p>
<ul>
<li>3D-FRONT (furnished rooms with semantics)</li>
<li>ICL-NUIM</li>
</ul>
<p>Fully supervised approaches are fruitful with synthetic data because full ground-truth available.</p>
<h5 id="sscnet">SSCNet</h5>
<p>RGB-D image -&gt; geometry occupancy grid (of fixed size) + semantic labels</p>
<p>Architecture: bunch of convolutions and <em>dilated convolutions</em>.</p>
<h4 id="scancomplete">ScanComplete</h4>
<p>(not completely clear what the point is)</p>
<ul>
<li>Handle arbitrary-size scenes</li>
<li>Trained on crops of scenes</li>
<li>complete the scan at different resolutions</li>
</ul>
<p>Also operates in auto-regressive fashion: (8 forward passes instead of one per voxel) <img src="autoregressive-scancomplete.png" alt="autoregressive-scancomplete.png" /></p>
<p>Note: on real-world scan data, the reconstruction is less clean</p>
<h4 id="learning-just-from-incomplete-real-world-data">Learning just from incomplete real-world data</h4>
<p>…in a self-supervised way, to get rid of the discrepancy between fake/real data.</p>
<h5 id="sg-nn-self-supervised-scan-complete-dai-et-al.-20">SG-NN: Self-supervised Scan Complete [Dai et al. ’20]</h5>
<p>Goal: learn to reconstruct patterns that are missing in a less complete scan, but present in a more complete scan =&gt; self-supervised scan completion!</p>
<ul>
<li>reconstructed target scan from several depth frames (still some holes in it)</li>
<li>other scan constructed from fewer frames -&gt; more holes</li>
<li>train reconstructing more complete frame from less complete frame (ignore space that is unobserved in the target scan in the loss)
<ul>
<li>note: if you don’t ignore the missing space, the network <em>learns to generate holes</em>. Otherwise it manages to fill holes.</li>
</ul></li>
</ul>
<figure>
<img src="sg-nn-unsupervised-completion.png" alt="sg-nn-unsupervised-completion.png" /><figcaption aria-hidden="true">sg-nn-unsupervised-completion.png</figcaption>
</figure>
<p>Multi-Scale approach as well: dense predictions at coarse level; more sparse predictions at upsampled level.</p>
<h5 id="spsg-self-supervised-color-generation-dai-et-al-21">SPSG: Self-supervised color generation [Dai et al ‘21’]</h5>
<p><a href="#sg-nn-self-supervised-scan-complete-dai-et-al-20">Like previous approach</a>, but also generate color.</p>
<p>Problems with simple loss like L1: many walls in the input =&gt; everything becomes wall-colored.</p>
<p>SPSG approach: project back to images (i.e. the actual input data); then use a 2d reconstruction loss, a 2d perceptual loss + a 2d adversarial loss.</p>
<figure>
<img src="spsg-losses.png" alt="spsg-losses.png" /><figcaption aria-hidden="true">spsg-losses.png</figcaption>
</figure>
<h4 id="leveraging-implicit-reconstruction-networks">Leveraging implicit reconstruction networks</h4>
<p>i.e. a technique that works well on 3D shapes -&gt; now applied to scenes? =&gt; doesn’t work as easily when working on scenes (not everything centered like for shapes, this makes it harder)</p>
<h5 id="local-implicit-grids-jiang-et-al.-20">Local implicit grids [Jiang et al. ‘20’]</h5>
<ul>
<li>Decompose space into smaller patches (where the patches should be: convolutional enc/dec. approach: then fine detail on patches via implicit approach)</li>
</ul>
<figure>
<img src="local-implicit-grids.png" alt="local-implicit-grids.png" /><figcaption aria-hidden="true">local-implicit-grids.png</figcaption>
</figure>
<p>Also an advantage: can reconstruct shapes it has never seen before (otherwise, if it has never seen a car, it will turn it into something that is not a car)</p>
<h5 id="convolutional-occupancy-network-peng-et-al.-20">Convolutional Occupancy Network [Peng et al. ‘20’]</h5>
<p>First convolutional, then implicit occupancy network at the end.</p>
<ul>
<li>Advantage: convolutions have translational invariance and can recognize non-centered patterns; implicit representations can reconstruct finer details.</li>
</ul>
<p>First: (coarse) occupancy voxel grid; convolutions =&gt; feature vector per voxel</p>
<p>Trilinear interpolation: interpolate voxel feature vectors into feature vector for particular point</p>
<p>Second: feed this feature vector of a point in a implicit occupancy network (shared between decoding locations)</p>
<h4 id="retrieval-as-reconstruction-exploiting-training-data-as-dictionary">Retrieval as Reconstruction (Exploiting training data as “dictionary”)</h4>
<p>Usually: condense training data into network weights. But we could leverage the more detailed training data during test time by using it as a dictionary to look up examples of nice constructed objects.</p>
<p>Advantages/Disadvantages of Retrieval as Reconstruction:</p>
<ul>
<li>For example: can be more sure that reconstructed objects are physically plausible (e.g. no chairs without legs etc.)</li>
<li>Disadvantage: usually no exact geometric matches.</li>
</ul>
<h5 id="retrievalfuse-siddiqui-et-al.-21">RetrievalFuse [Siddiqui et al. ’21]</h5>
<p>Idea: create initial reconstruction estimate by composing chunks of train data. Then make it consistent afterwards</p>
<p>QUESTION: how well does this work with unseen objects?</p>
<p>Database retrieval (k-NN) -&gt; attention-based refinement -&gt; reconstruction</p>
<figure>
<img src="retrievalfuse.png" alt="retrievalfuse.png" /><figcaption aria-hidden="true">retrievalfuse.png</figcaption>
</figure>
<p>kNN via embedding space. Constructed with <em>deep metric learning</em>:</p>
<ul>
<li>a point <em>f</em> should be close to similar points <em>f+</em></li>
<li>…and far away from dissimilar points <em>f-</em></li>
<li>done one triple (<em>f, f+, f-</em>) at a time</li>
</ul>
<p>Use dot product similarity in more complicated expression to compute loss</p>
<p>k NNs merged together with attention.</p>
<h5 id="scan2cad-avetisyan-et-al.-19">Scan2CAD [Avetisyan et al. ’19]</h5>
<p>Reconstruct scene by aligning CAD models (e.g. ShapeNet models) to it</p>
<p>For a point, estimate a heatmap on the candidate CAD models on where this point is likely to be.</p>
<p>Scan input, point, + candidate CAD models -&gt; Encoded by 3D convs -&gt; output: match (0 or 1), heatmap over CAD model, scale</p>
<p>Problem: objects aligned independently of each other.</p>
<h5 id="scenecad-avetisyan-et-al.-20">SceneCAD [Avetisyan et al. ’20]</h5>
<p>Take into account dependency of objects on each other via a GraphNN (used at train time)</p>
<h3 id="scene-synthesis-task">Scene synthesis task</h3>
<h5 id="wang-et-al.-18">[Wang et al. ’18]</h5>
<p>Trick: create scene by iteratively adding objects to a room (easier to generate everything from scratch). It must be learned e.g. which objects appear together usually.</p>
<p>Loop: partial scene -&gt; decide: continue adding objects? (CNN) -&gt; if yes: predict object category + location (CNN) and place object -&gt; repeat</p>
<p>(autoregressive approach because of the loop)</p>
<h3 id="textured-scene-generation">Textured Scene Generation</h3>
<p>For content creation and visualization: need more than geometry; also textures, materials, lightning…</p>
<h4 id="texture-optimization-task">Texture optimization task</h4>
<p>Assume texture is already known (from images), but from motion blur etc., we get blur in the reconstruction. (Challenges: camera pose estimation, motion blur, distortion artifacts, view-dependent materials)</p>
<h5 id="dlite-huang-et-al.-17">3DLite [Huang et al. ‘17’]</h5>
<p>Use simple geometric primitives (i.e. planes) and project high-res textures onto these.</p>
<figure>
<img src="3DLite.png" alt="3DLite.png" /><figcaption aria-hidden="true">3DLite.png</figcaption>
</figure>
<p>Apparently, uses more classical algorithms mostly. Also, kind of a “handcrafted” pipeline</p>
<h5 id="adversarial-texture-optimization-huang-et-al.-20">Adversarial Texture Optimization [Huang et al. ’20]</h5>
<p>Leraning an adversarial objective function that knows how real textures look.</p>
<figure>
<img src="adversarial-texture-optimization.png" alt="adversarial-texture-optimization.png" /><figcaption aria-hidden="true">adversarial-texture-optimization.png</figcaption>
</figure>
<p>In some way use differently aligned perspectives to feed into the discriminator (details a bit unclear)</p>
<h1 id="functional-analysis-of-scenes">10 - Functional Analysis of Scenes</h1>
<p>Understanding possible interactions with objects (i.e. their <em>function</em>)</p>
<p>Not the object <em>type</em>, but more: e.g. to know how to open a drawer, we also need to find its handle. Which parts are the interactable parts?</p>
<p>Examples:</p>
<ul>
<li>pushcart: <em>wheel</em> - spinning -&gt; allows movement</li>
<li>fan: <em>rotor</em> - spinning -&gt; allows to blow air</li>
<li>what pose would a person be in when using a hammer/a banana/binoculars?</li>
</ul>
<h3 id="subtask-predicting-interactions">Subtask: Predicting interactions</h3>
<p>Related: predicting interaction poses (which pose does a person have when watching TV/writing on a whiteboard etc.). Can have a time component as well.</p>
<p>(Example: video with a horse -&gt; can we forecast what the woman does next?)</p>
<h5 id="challenges-1">Challenges</h5>
<ul>
<li>data acquisition: how do we get enough and diverse enough data for such a task?</li>
<li>how to represent objects/scene/person? (Maybe a graph?)</li>
</ul>
<h5 id="pigraphs-savva-et-al.-16">PiGraphs [Savva et al. ’16]</h5>
<p>Tracked human actors with motion capture. Learnt graph representation:</p>
<ul>
<li>each body part (skeleton part) and each object part becomes a node</li>
<li>goal: learn co-occurence of these (e.g. “hip joint &lt;-&gt; sitting &lt;-&gt; chairs”)</li>
</ul>
<p>based on input texts (“watch-TV+restfeet-stool”), generate interaction snapshot. <img src="pigraphs.png" alt="pigraphs.png" /></p>
<p>Limitation: very limited because of the small training set.</p>
<h3 id="human-motion-extraction">Human motion extraction</h3>
<h5 id="imapper-monspart-et-al.-18">iMapper [Monspart et al. ’18]</h5>
<p>Get RGB video as input; Generates synthesized scene with human motion on a skeleton. <img src="iMapper-input-output.png" alt="iMapper-input-output.png" /></p>
<p>Training: with short interaction videos (<em>scenelests</em>) from PiGraph data. First fit the skeleton statically, then perform a <em>scenelet fitting</em> using the database, then refine.</p>
<h3 id="generating-human-object-interaction-snapshots">Generating Human-Object interaction snapshots</h3>
<h5 id="table-object-interaction-generation-wang-et-al.-19">Table object interaction generation [Wang et al. ’19]</h5>
<p>Train on interactions of a guy sitting at a table; using off-the-shelf image object recognition. Then synthesize new interactions to get animations of objects moving around.</p>
<h5 id="predicting-pose-snapshots-li-et-al.-19">Predicting pose snapshots [Li et al. ’19]</h5>
<p>Predicting static pose snapshots that showcase how the objects can be used.</p>
<p>extract</p>
<ul>
<li>semantic knowledge (skeletons of persons in videos)</li>
<li>geometry knowledge</li>
</ul>
<p>=&gt; predict skeletons poses in 3d environment</p>
<h5 id="generating-3d-people-in-scenes-zhang-et-al.-20">Generating 3D People in Scenes [Zhang et al. ’20]</h5>
<p>Scene given. Generate bodies with VAE, use SMPL-X 3D body representation. Then fit the human body to the scene to avoid floating/collisions.</p>
<p>(no architecture details given)</p>
<h5 id="populating-3d-scenes-hassan-et-al.-21">Populating 3D Scenes [Hassan et al. ’21]</h5>
<p>More or less trying to achieve the same as the previous one (also uses SMPL-X).</p>
<h3 id="predicting-human-motion">Predicting human motion</h3>
<figure>
<img src="predict-human-motion.png" alt="predict-human-motion.png" /><figcaption aria-hidden="true">predict-human-motion.png</figcaption>
</figure>
<h5 id="human-motion-prediction-with-scene-context-cao-et-al.-20">Human motion prediction with scene context [Cao et al. ’20]</h5>
<p>Input: image + 2D pose history, Sample possible 2D destinations and predict the 3D path to the destinations.</p>
<p>Synthetic data generated from GTA game (persons moving in scene).</p>
<ol type="1">
<li>predict goal where person is moving (<em>GoalNet</em>)
<ul>
<li>heatmap over possible locations</li>
</ul></li>
<li>predict single trajectory - i.e. where the hip moves, in 2D, without moving the full skeleton (<em>PathNet</em>)
<ul>
<li>set of 2d heatmaps over time</li>
</ul></li>
<li>predict movement of all skeleton joints (<em>PoseNet</em>)
<ul>
<li>transformer: start with 2d trajectory -&gt; project to 3d as noisy input; then apply transformer to refine.</li>
<li>attention on pose level: attends over previous poses</li>
</ul></li>
</ol>
<p>Excursion: transformers and attention (see slides</p>
<h5 id="forecasting-characteristic-poses-diller-et-al.-21">Forecasting Characteristic Poses [Diller et al. ’21]</h5>
<p>Goal: predict not just random poses, but characteristic poses of certain actions. Specifically, start with an input pose and simulate the next poses over time when a certain action is performed.</p>
<p>Multi-modal prediction distribution: e.g. left hand maybe be passing or close to body (if the right hand is passing). Predicting the average of both is bad =&gt; auto-regressive prediction: predict joints in sequential order.</p>
<p>Achieved using attention (over previous predictions). Then decodes into 3d heatmap (aka probability distribution).</p>
<figure>
<img src="forecasting-characteristic-poses.png" alt="forecasting-characteristic-poses.png" /><figcaption aria-hidden="true">forecasting-characteristic-poses.png</figcaption>
</figure>
<h3 id="simulation-environments">Simulation Environments</h3>
<p>Virtual 3D environments for training/testing AI agents</p>
<ul>
<li>no tedious real-world data collection and no material cost</li>
<li>simulate rare/dangerous scenarios</li>
<li>easier to reproduce stuff</li>
</ul>
<h5 id="habitat">Habitat</h5>
<p>3D agent simulator (efficient rendering, on existing 3D datasets)</p>
<p>Has benchmark tasks:</p>
<ul>
<li>PointNav: agent at random position in unseen environment must navigate to target position</li>
<li>ObjectNav: agent at random position in unseen environment must find instance of an object category</li>
</ul>
<h5 id="ai2thor">AI2Thor</h5>
<p>Basic physics, partly dynamic objects</p>
<h5 id="gibson">Gibson</h5>
<p>Works with scanned data.</p>
<h5 id="sapien">SAPIEN</h5>
<p>PartNet has some objects with annotated parts where motion parameters (how far can it move) are specified. SAPIEN focuses on simulating interaction with such objects.</p>
<h1 id="weak-supervision-n-shot-learning-data-efficiency">11 - Weak Supervision, n-shot Learning, Data Efficiency</h1>
<p>Broad motivation for this chapter: We want to use as little (annotated) data as possible, because data collection and data annotation are expensive.</p>
<h3 id="training-methods-and-required-amount-of-labeled-data">Training methods and required amount of labeled data</h3>
<ul>
<li><em>Supervised</em>: manually labeled data by expert annotators (expensive)</li>
<li><em>Unsupervised</em>: no annotations at all (learn structural patterns)</li>
<li><em>Semi-Supervision</em>: both a labeled and an unlabeled set of data</li>
<li><em>Weak Supervision</em>: lower quality labels that you can get without expert annotators</li>
<li><em>Self-Supervision</em>: Automatically generate supervision signal</li>
<li><em>Transfer Learning</em>: transfer pretrained models to your task</li>
</ul>
<p>Other hybrids exist: e.g. active learning -&gt; use annotations more efficiently (human in the loop)</p>
<h3 id="few-shot-learning">Few-shot learning</h3>
<p>See only few (or one) example per class.</p>
<h5 id="reconstruction-from-image-on-unseen-classes-zhang-et-al.-18">Reconstruction from image on unseen classes [Zhang et al. ’18]</h5>
<p>Problem usually: if you train on tables and chairs then want to reconstruct a bed, it might end up looking like a table. Goal in order to avoid: “the model should memorize as little as possible”</p>
<p>Input image -&gt; depth estimation -&gt; 2d spherical map (geometry outprojected into a sphere) -&gt; inpaint spherical map image to fill in missing geometry -&gt; backproject to 3d shape -&gt; refine to get a final 3d shape</p>
<figure>
<img src="reconstruction-on-unseen-classes.png" alt="reconstruction-on-unseen-classes.png" /><figcaption aria-hidden="true">reconstruction-on-unseen-classes.png</figcaption>
</figure>
<h5 id="learning-category-specific-mesh-reconstruction-kanazawa-et-al.-18">Learning Category-Specific Mesh Reconstruction [Kanazawa et al. ’18]</h5>
<p>Input: lots of annotated images of one specific class (e.g. birds), but no 3d information. Then for one specific image, reconstruct a textured mesh.</p>
<p>(didn’t get the details of the architecture)</p>
<figure>
<img src="bird-mesh-reconstructed.png" alt="bird-mesh-reconstructed.png" /><figcaption aria-hidden="true">bird-mesh-reconstructed.png</figcaption>
</figure>
<h5 id="few-shot-single-image-reconstruction-wallace-and-hariharan-19">Few-shot single image reconstruction [Wallace and Hariharan ’19]</h5>
<p>Transform an input image to a 3d representation, but this time a <em>category prior</em> can be used (e.g. image of a sofa, and the category prior is the mean of all known sofas).</p>
<p>Trained on image/3d shape pairs. But at test time, novel categories are presented where there are only a few (say 25) examples.</p>
<p>Architecture: encode image with 2d convolutions, prior shape with 3d convolutions -&gt; concatenate (or sum?) the codes -&gt; decode back with transposed convoutions.</p>
<p>The limitation is of course that some categories have much diversity, and then the approach doesn’t work as well.</p>
<figure>
<img src="sofa-category-prior.png" alt="sofa-category-prior.png" /><figcaption aria-hidden="true">sofa-category-prior.png</figcaption>
</figure>
<h3 id="generalizing-accross-datasets">Generalizing accross datasets</h3>
<p>“Train in Germany, Test in the USA”. An issue e.g. in autonomous driving: if an object detector is trained in different circumstances than it used in. If applying such a model to the different test dataset blindly, there will be a performance gap that we want to avoid.</p>
<h5 id="wang-et-al.-20">[Wang et al. ’20]</h5>
<p>Frequent issues found in self-driving car scenarios: misdetection (detect something where there isn’t anything), but even more mislocalization. The latter comes from different car sizes in e.g. Germany vs. the US, or accross different cities etc.</p>
<p>Solved by data normalization (<em>domain adaption</em>).</p>
<h3 id="approaches-with-lessno-supervision">Approaches with less/no supervision</h3>
<h5 id="discovery-of-latent-3d-keypoints-suwajanakorn-et-al.-18">Discovery of Latent 3D Keypoints [Suwajanakorn et al. ’18]</h5>
<p>Given: multi-view image observations. Goal: find keypoints in 3D without having annotated training data. (Related: annotating 3D Keypoints is not well-defined anyway but very much subjective).</p>
<p>Use auxiliary task as weak supervisory signal: Aux. task is pose estimation from an image. Given are two views of which the ground-truth transformation between them is known . Then as supervision, they use that transforming the keypoints output for the first image should be close to the keypoints output for the second image</p>
<p>This matches what you want from a keypoint: Keypoints should be points that are identifiable from several views of the shape.</p>
<figure>
<img src="keypoint-detection.png" alt="keypoint-detection.png" /><figcaption aria-hidden="true">keypoint-detection.png</figcaption>
</figure>
<h5 id="learning-the-depths-of-moving-people-li-et-al.-19">Learning the Depths of Moving People [Li et al. ’19]</h5>
<p>Input: Youtube videos of people standing still. Goal: estimate depth maps.</p>
<p>Using the “mannquin challenge” dataset (2000 videos): people try to stand still like mannequins, which satisfies the static scene assumption quite well. Via structure from motion and multiview stereo a depth image which serves as supervision signal ist estimated.</p>
<p>Goal at test time: also apply it to moving people, not just static people. To improve for this, also take into account a mask that shows where the humans are and a depth from flow estimation (obtained from having a video, not only a static image).</p>
<figure>
<img src="mannequin-challenge.png" alt="mannequin-challenge.png" /><figcaption aria-hidden="true">mannequin-challenge.png</figcaption>
</figure>
<h5 id="sg-nn-self-supervised-scan-completion-dai-et-al.-20">SG-NN: Self-supervised scan completion [Dai et al. ’20]</h5>
<p>Mentioned before <a href="#sg-nn-self-supervised-scan-complete-dai-et-al-20">in Lecture 9</a>. This also uses self-supervision to complete scans, while having only a dataset of incomplete scans, by learning how to go from less to more complete.</p>
<h5 id="pointcontrast-xie-et-al.-20">PointContrast [Xie et al. ’20]</h5>
<p>Goal: semantic understanding of point clouds. Pretrained on large unannotated dataset, fine-tuned on small dataset.</p>
<p>These idea come from similar ideas in 2D representation learning:</p>
<h6 id="excursion-2d-representation-learning">Excursion: 2D Representation Learning</h6>
<p>SimCLR [Chen et al. ’20], MoCo [He et al. ’20]. Use <em>contrastive loss function</em> to learn a representation where similar things are close, and dissimilar things are far away from each other. Compare with <a href="#retrievalfuse-siddiqui-et-al-21">RetrievalFuse</a>, where a similar concept and loss were used.</p>
<p>The supervisory signal here is that we can generate more data samples we know to be similar by <em>data augmentation</em> (If you have an image of a cat and do some cropping and resizing, you know it’s still an image of a cat; so make sure that the model learns this).</p>
<h6 id="similar-ideas-used-in-pointcontrast">Similar ideas used in PointContrast</h6>
<p>In 3D, we can even use more augmentation techniques related to multi-view constraints. So pretrain the model to be able to recognize originally close points in 2 images of the same 3d scene as similar, and far away points as dissimilar.</p>
<h5 id="data-efficient-3d-scene-understanding-hou-et-al.-21">Data Efficient 3D Scene understanding [Hou et al. ’21]</h5>
<p>Extension of the previous PointContrast idea. Partition the scene into smaller regions, apply the contrastive loss in each region separately. This allows to get more performance out of using more sample points (while performance of the vanilla PointContrast approach saturates).</p>
<p>After the pretraining, train on limited number of fully-labeled scenes and on all scenes with limited point labeling budget (this would be suited to <em>active labeling</em>; not actually used here though).</p>
<h3 id="domain-adaption">Domain Adaption</h3>
<p>Scenario: Much labeled data in one domain, but much less in a second domain. One example: synthetic domain (object annotations for free) &lt;-&gt; real domain (annotations expensive).</p>
<p>Common tasks approached: semantic segmentation, object detection.</p>
<h4 id="excursion-domain-adaption-in-the-2d-domain">Excursion: Domain adaption in the 2D domain</h4>
<h5 id="cycada-cycle-consistent-adversarial-domain-adaption-hoffman-et-al.-18">CyCADA: Cycle-Consistent Adversarial Domain Adaption [Hoffman et al. ’18]</h5>
<p>Adapt between synthetic GTA images and real CityScapes images of cars on roads.</p>
<p>GAN: Generator should stylize the source image similar to the target image. Discrimnator tries to tell apart the images and also the extracted features. The source image and stylized source image should have semantically consistent features. Then goal: be able to use the same feature extractors from the source domain on the target domain.</p>
<h4 id="domain-adaption-in-3d">Domain Adaption in 3D</h4>
<p>No details here, but the same thing as for 2D applies: We have synthetic data that is labeled already, and want to use it for pretraining and then be able to apply the models to real data.</p>
</body>
</html>
